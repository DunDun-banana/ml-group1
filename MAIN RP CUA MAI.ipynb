{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4ee4fd09",
      "metadata": {
        "id": "4ee4fd09"
      },
      "source": [
        "### I. Libary import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b72ccedf",
      "metadata": {
        "id": "b72ccedf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from clearml import Task\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import optuna\n",
        "\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import optuna.visualization as vis\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "from clearml import Logger\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.base import clone\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "# from xgboost import XGBRegressor\n",
        "\n",
        "from src import data_preprocessing as dp\n",
        "from src import pipeline as pl\n",
        "from src import new_feature_engineering_daily as fe\n",
        "from src import feature_selection as fs\n",
        "from src.model_evaluation import evaluate, evaluate_multi_output\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
        "\n",
        "SEED = 42 # vẫn phải chọn random của model đấy\n",
        "\n",
        "# Python, NumPy\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Hash seed cho Python interpreter (ảnh hưởng tới dict order)\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "\n",
        "# Giới hạn luồng tính toán song song (để tránh floating-point nondeterminism)\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
        "\n",
        "# tạo sampler của Optuna có seed cố định\n",
        "sampler = optuna.samplers.TPESampler(seed=SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c965d253",
      "metadata": {
        "id": "c965d253"
      },
      "source": [
        "### I. Data prepare"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6yWJckbZ1lsS",
      "metadata": {
        "id": "6yWJckbZ1lsS"
      },
      "source": [
        ".."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e8a9465b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8a9465b",
        "outputId": "2c30ea4e-1217-4985-f2c9-56d7a4003695"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Loaded data with shape: (3660, 33)\n",
            "Dropped column: 'description'\n",
            "Dropped column: 'icon'\n",
            "Dropped column: 'stations'\n",
            "Dropped column: 'name'\n"
          ]
        }
      ],
      "source": [
        "# 1. Load raw Data\n",
        "df = dp.load_data(r'data/raw data/Hanoi Daily 10 years.csv')\n",
        "\n",
        "# 2. basic preprocessing for all data set\n",
        "df = dp.basic_preprocessing(df=df)\n",
        "\n",
        "# 3. Chia train, test theo thời gian (80/20)\n",
        "train_size = 0.8\n",
        "n = len(df)\n",
        "\n",
        "train_df = df.iloc[:int(train_size * n)]\n",
        "test_df = df.iloc[int(train_size * n):]\n",
        "\n",
        "# 4. Fit preprocessing pipeline trên train\n",
        "pipeline1 = pl.build_preprocessing_pipeline()\n",
        "pipeline1.fit(train_df)\n",
        "\n",
        "# 5. Transform cả train và test\n",
        "train_processed = pipeline1.transform(train_df)\n",
        "test_processed = pipeline1.transform(test_df)\n",
        "\n",
        "# 6. Chia train, test theo thời gian (80/20)\n",
        "train_size = 0.8\n",
        "n = len(df)\n",
        "\n",
        "train_df = df.iloc[:int(train_size * n)]\n",
        "test_df = df.iloc[int(train_size * n):]\n",
        "\n",
        "# 7. Fit preprocessing pipeline trên train\n",
        "pipeline1 = pl.build_preprocessing_pipeline()\n",
        "pipeline1.fit(train_df)\n",
        "\n",
        "# 8. Transform cả train và test\n",
        "train_processed = pipeline1.transform(train_df)\n",
        "test_processed = pipeline1.transform(test_df)\n",
        "\n",
        "# 9. Lưu lại pipeline 1\n",
        "# joblib.dump(pipeline1, r\"pipelines\\preprocessing_pipeline.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ef4631b",
      "metadata": {
        "id": "4ef4631b"
      },
      "source": [
        "### II. Feature Engineering and Training Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1d2c53a",
      "metadata": {
        "id": "a1d2c53a"
      },
      "source": [
        "## 2.1 OVERVIEW"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc83d641",
      "metadata": {
        "id": "bc83d641"
      },
      "source": [
        "### **2.1.1. Forecasting Architectures**\n",
        "\n",
        "#### **1. Recursive Strategy**\n",
        "**Method:**  \n",
        "A single model is trained to predict only one step ahead (t + 1).  \n",
        "To forecast t + 2, the model’s own prediction for t + 1 is fed back into its input features.  \n",
        "This process is repeated for each subsequent step.\n",
        "\n",
        "**Drawback:**  \n",
        "This approach is highly susceptible to **error accumulation** — any small error in the t + 1 forecast becomes part of the input for t + 2, and the errors compound over time.  \n",
        "Performance therefore degrades quickly for longer horizons.\n",
        "\n",
        "**When to use:**  \n",
        "Useful for short-term forecasts where the system dynamics are stable and small accumulated errors have minimal effect.\n",
        "\n",
        "#### **2. Multi-Output Strategy**\n",
        "**Method:**  \n",
        "A single, more complex model is trained to predict multiple future time steps (t + 1 → t + 5) simultaneously in one forward pass.\n",
        "\n",
        "**Drawback:**  \n",
        "This model acts as a **generalist** — while computationally efficient, it often struggles to optimize each specific horizon, producing good but not optimal results for any particular forecast step.\n",
        "\n",
        "**When to use:**  \n",
        "When efficiency and simultaneous multi-step outputs are more important than maximizing accuracy at a specific horizon.\n",
        "\n",
        "#### **3. Direct Strategy *(Chosen Approach)***\n",
        "**Method:**  \n",
        "Train a **separate, independent model** for each forecast horizon.  \n",
        "For example, the model for t + 3 is trained exclusively on data pairs of *(Features at time t, Target at time t + 3)*.  \n",
        "In our case, we train **five separate models**, one for each horizon (t + 1 → t + 5).\n",
        "\n",
        "**Justification:**  \n",
        "This strategy is the **most robust and accurate** for our problem.  \n",
        "It avoids recursive error propagation and allows each model to specialize in the unique temporal relationships relevant to its forecast distance.  \n",
        "Although it requires training multiple models, the gain in accuracy and reliability justifies the extra computational cost, especially since our dataset per day is relatively small.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "857952ab",
      "metadata": {
        "id": "857952ab"
      },
      "source": [
        "### **2.1.2. Metrics Explanation (MAE, RMSE, R²)**\n",
        "\n",
        "In this project, we evaluate our weather forecasting models using three main regression metrics: **Mean Absolute Error (MAE)**, **Root Mean Squared Error (RMSE)**, and the **Coefficient of Determination (R²)**.  \n",
        "Each metric provides a different perspective on model performance and together they give a more complete evaluation.\n",
        "\n",
        "#### **1. Mean Absolute Error (MAE)**\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "$$\n",
        "MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
        "$$\n",
        "\n",
        "**Meaning:**  \n",
        "MAE measures the **average absolute difference** between predicted and actual values.  \n",
        "It tells us, on average, how much the predictions deviate from the true observations — regardless of direction (positive or negative).\n",
        "\n",
        "**Characteristics:**\n",
        "- Interpretable in the same unit as the target variable (e.g., °C for temperature).\n",
        "- Less sensitive to large outliers.\n",
        "- Easier to interpret and explain to non-technical audiences.\n",
        "\n",
        "#### **2. Root Mean Squared Error (RMSE)**\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "$$\n",
        "RMSE = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 }\n",
        "$$\n",
        "\n",
        "**Meaning:**  \n",
        "RMSE measures the **square root of the average squared error**.  \n",
        "It penalizes larger errors more heavily than MAE, which makes it useful for emphasizing cases where the model performs particularly poorly.\n",
        "\n",
        "**Characteristics:**\n",
        "- Sensitive to outliers and large deviations.  \n",
        "- Provides stronger penalty for big mistakes → better for highlighting unstable models.\n",
        "- Has the same unit as the target variable (°C).\n",
        "\n",
        "\n",
        "#### **3. Coefficient of Determination (R² Score)**\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "$$\n",
        "R^2 = 1 - \\frac{ \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 }{ \\sum_{i=1}^{n}(y_i - \\bar{y})^2 }\n",
        "$$\n",
        "\n",
        "**Meaning:**  \n",
        "R² represents the **proportion of variance** in the target variable that can be explained by the model.  \n",
        "The closer R² is to 1, the better the model explains the data variability.\n",
        "\n",
        "**Interpretation:**\n",
        "- \\( R^2 = 1 \\): Perfect prediction.  \n",
        "- \\( R^2 = 0 \\): Model performs no better than simply predicting the mean.  \n",
        "- \\( R^2 < 0 \\): Model performs worse than predicting the mean.\n",
        "\n",
        "\n",
        "#### **4. Implementation in Our Project**\n",
        "\n",
        "We use the function `evaluate_multi_output()` defined in the `model_evaluation.py` file to compute these metrics.  \n",
        "**Inputs:**  \n",
        "- `y_true`: True target values (actual weather data).  \n",
        "- `y_pred`: Predicted values from our trained models.  \n",
        "\n",
        "**Outputs:**  \n",
        "- Dictionary containing `MAE`, `RMSE`, and `R²` scores for each forecast horizon.\n",
        "\n",
        "This allows us to evaluate each of our five direct models (t+1 → t+5) consistently and compare their performance.\n",
        "\n",
        "\n",
        "#### **5. Metric Prioritization**\n",
        "\n",
        "Although all metrics are reported for completeness, our group **prioritizes RMSE** for model optimization.\n",
        "\n",
        "**Reasoning:**\n",
        "- RMSE strongly penalizes large prediction errors, which is important for weather forecasting where a few extreme wrong predictions (e.g., 5–6°C off) can be more harmful than small consistent ones.\n",
        "- RMSE thus encourages the model to perform more consistently and handle variability better.\n",
        "\n",
        "During evaluation, we still analyze **MAE** and **R²** alongside RMSE to ensure a balanced understanding of model accuracy and reliability.\n",
        "\n",
        "\n",
        "**➡️ In short:**  \n",
        "We evaluate with all three metrics but **optimize for RMSE** to achieve a more stable and accurate forecasting performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f5cf312",
      "metadata": {
        "id": "2f5cf312"
      },
      "source": [
        "### **2.1.3. Cách đánh giá**\n",
        "Ngoài việc chia train, test thì trong quá trình tuning và so sánh giữa các phương pháp mình sẽ sử dụng Cross-Val trên train. Vẽ cho t cái biểu đồ tiếp (nếu biểu đồ tổng quát có cụ thể rồi thì thôi)\n",
        "\n",
        "\n",
        "Đây là đoạn t viết để giải thích chung chung, (ai đọc thì check xem t vt đúng luôn không) cho mọi người. Người viết tóm tắt ngắn gọn lại quy trình thôi\n",
        "\n",
        "- Workflow: Trong quá trình so sánh các phương án, tuning chỉ dùng dữ liệu trên tập train (tập này sẽ được chia thành 5 cross-validation - tức 5 cặp train/ test nhỏ - và mình sẽ đánh giá mức độ dự đoán và ổn định của model trên các cặp train/ test này)\n",
        "\n",
        "=> Tìm được phương án tốt nhất rồi thì mới training lại và đánh giá lần cuối trên toàn bộ train và test set.\n",
        "\n",
        "- Phần so sánh việc drop base, hay xử lí cate: Mấy bạn viết report kết quả, Ngoài việc sử dụng (Metrics trung bình của 5 tập hay Std của 5 tập - thể hiện mức độ biến động/ ổn định giữa các fold) tìm thêm biểu đồ biểu như kiểu box plot (thầy Long) để trực quan hoá, nhớ là kèm giải thích ngắn gọn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf28fe2c",
      "metadata": {
        "id": "cf28fe2c"
      },
      "source": [
        "### **2.1.4. Nói qua Công cụ Tuning mình dùng - Nhàn**\n",
        "- Optuna để tuning\n",
        "- ClearML để lưu kết quả: Chụp ảnh màn hình, xong về mục artifact mình sẽ log những gì, scalar log những gì, ... Thêm link CLearML vào đây\n",
        "\n",
        "Nhàn lên được ClearML nên viết phần này"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a0c2406",
      "metadata": {
        "id": "5a0c2406"
      },
      "source": [
        "## **2.2 Feature engineering and Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f99e4980",
      "metadata": {
        "id": "f99e4980"
      },
      "source": [
        "### **2.2.1. Linear Model**\n",
        "### MNG ĐỌC\n",
        "- Đang để tune riêng 5 model, Mà kết quả tune nó hơi lạ, Nhàn tune Linear (Ridge) tiếp cho t, kết quả bất lực quá thì thôi tune để lấy số liệu, với biểu đồ so sánh giúp mình đưa ra kết luận là chọn Tree(lgb) thôi\n",
        "\n",
        "- Hoàng chịu khó cop lại rồi thay lgb vào hoặc viết mới, sửa lại các kiểu tuỳ\n",
        "\n",
        "- Ai tune thì chú ý cũng chưa có mấy cái biểu đồ hình vẽ  như vis.plot_parallel_coordinate(study_grad), vis.plot_param_importances(study=study_grad), vis.plot_slice(study=study_grad). Mng cho thêm vào, tại mk cũng sẽ cần cho 1-2 cái vào report để nói về quá trình tune + kèm giải thích ngắn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24089e2c",
      "metadata": {
        "id": "24089e2c"
      },
      "outputs": [],
      "source": [
        "# Đây là mấy cái biểu đồ cũ lúc tuning mình dùng, mọi người tự ghép vào\n",
        "\n",
        "# === Vẽ biểu đồ RMSE theo trial  ===\n",
        "# fig3 = plt.figure(figsize=(7, 4))\n",
        "# plt.plot([t.value for t in study_grad.trials])\n",
        "# plt.xlabel(\"Trial\")\n",
        "# plt.ylabel(\"RMSE\")\n",
        "# plt.title(\"Optuna RMSE per Trial\")\n",
        "\n",
        "# logger_grad.report_matplotlib_figure(\n",
        "#     title=\"Optuna Performance\",\n",
        "#     series=\"RMSE Curve\",\n",
        "#     figure=fig3,\n",
        "#     iteration=len(study_grad.trials)\n",
        "# )\n",
        "# plt.close(fig3)\n",
        "\n",
        "# vis.plot_parallel_coordinate(study_grad)\n",
        "# vis.plot_slice(study=study_grad)\n",
        "# vis.plot_param_importances(study=study_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ade5ad12",
      "metadata": {
        "id": "ade5ad12"
      },
      "source": [
        "### **2.2.2. Giới thiệu Ridge**\n",
        "\n",
        "Ridge Regression is a type of **linear regression** that includes **L2 regularization**, which helps prevent overfitting by penalizing large model coefficients.  \n",
        "In the context of weather prediction, Ridge is an appropriate baseline model because it can capture the **linear relationships** between meteorological factors (such as temperature, humidity, and pressure) while remaining stable even when features are correlated.\n",
        "\n",
        "\n",
        "#### Key Characteristics\n",
        "- **Simple and interpretable:** Ridge provides direct insight into how each feature affects the prediction.  \n",
        "- **Regularization (L2 penalty):** Prevents the model from assigning excessively high weights to correlated or noisy features.  \n",
        "- **Good with multicollinearity:** Works well when predictors (like temperature, humidity, and radiation) are correlated — a common situation in weather data.  \n",
        "- **Fast training:** Computationally efficient, making it suitable for initial modeling or feature selection stages.\n",
        "\n",
        "#### Why Ridge Fits Our Forecasting Goal\n",
        "For our dataset on Hanoi weather, Ridge serves as a **strong linear baseline**:\n",
        "- It captures direct linear dependencies among weather indicators.  \n",
        "- It benefits from the engineered interaction features and scaling pipeline built earlier.  \n",
        "- It helps identify which features have the most consistent influence on temperature or weather conditions.\n",
        "\n",
        "Although more advanced models (like LightGBM) may later achieve better accuracy, Ridge Regression provides a **robust, explainable, and well-regularized foundation** for understanding the structure of our data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c24a315",
      "metadata": {
        "id": "0c24a315"
      },
      "source": [
        "- Đoạn linear này nói chung là Sương mai làm cho t bản tóm tắt kết quả lại, xong kết luận luôn là mình sẽ chọn tree base (LGB), còn bên dưới là quá trình chạy thử, tuning các thứ nếu thầy muốn xem"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11661cdb",
      "metadata": {
        "id": "11661cdb"
      },
      "source": [
        "### 1.2. Input preparation (Feature Engineering and Selection) for linear model (Ridge)  - Sương Mai, Nhàn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "923009d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "923009d9",
        "outputId": "41b35bad-f7d4-42fe-98f9-de679903ab12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đã tạo 7 biến interaction\n",
            "Đã tạo tổng cộng 78 lag features.\n",
            "Đã tạo 84 rolling features.\n",
            "Đã tạo 7 biến interaction\n",
            "Đã tạo tổng cộng 78 lag features.\n",
            "Đã tạo 84 rolling features.\n",
            "Train: ((2893, 264), (2893, 5)), Test: ((697, 264), (697, 5))\n",
            "\n",
            "Kiểm tra còn biến object hay category 0\n",
            "\n",
            "Drop Base\n",
            "số lượng Trước Khi drop base (2893, 264) (697, 264)\n",
            "số lượng sau drop base (2893, 243) (697, 243)\n"
          ]
        }
      ],
      "source": [
        "## lưu ý khi tune linear này sẽ khác so với LGB\n",
        "\n",
        "# 1. có sử dụng các biến interaction: is_linear = True\n",
        "\n",
        "# Tạo feature engineering (drop NaN sau khi rolling/lag)\n",
        "train_feat, target_col = fe.feature_engineering(train_processed, is_drop_nan= True, is_linear= True)\n",
        "test_feat, _ = fe.feature_engineering(test_processed, is_drop_nan= True, is_linear= True)\n",
        "\n",
        "# Chia X, y riêng biệt\n",
        "X_train = train_feat.drop(columns= target_col)\n",
        "y_train = train_feat[target_col]\n",
        "\n",
        "X_test = test_feat.drop(columns= target_col)\n",
        "y_test = test_feat[target_col]\n",
        "\n",
        "\n",
        "# 2. Encoding sang numeric\n",
        "encoder = pl.build_encoding_pipeline(is_category= False)\n",
        "encoder.fit(X_train, X_train['temp'])\n",
        "\n",
        "X_train = encoder.transform(X_train)\n",
        "X_test = encoder.transform(X_test)\n",
        "\n",
        "\n",
        "print(f\"Train: {X_train.shape, y_train.shape}, Test: {X_test.shape, y_test.shape}\")\n",
        "\n",
        "cat_features = X_train.select_dtypes(include=['category','object']).columns\n",
        "print('\\nKiểm tra còn biến object hay category', len(cat_features))\n",
        "\n",
        "\n",
        "# 3. Drop base\n",
        "print('\\nDrop Base')\n",
        "print('số lượng Trước Khi drop base', X_train.shape, X_test.shape)\n",
        "X_train = fe.drop_base_features(X_train)\n",
        "X_test = fe.drop_base_features(X_test)\n",
        "print('số lượng sau drop base', X_train.shape, X_test.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84f1248d",
      "metadata": {
        "id": "84f1248d"
      },
      "source": [
        "#### 1.3. Tuning trên cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e30d71f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "e30d71f6",
        "outputId": "295118b6-85f1-4ef5-852c-4debf617cfc9"
      },
      "outputs": [
        {
          "ename": "MissingConfigError",
          "evalue": "It seems ClearML is not configured on this machine!\nTo get started with ClearML, setup your own 'clearml-server' or create a free account at https://app.clear.ml\nSetup instructions can be found here: https://clear.ml/docs",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMissingConfigError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Task\u001b[38;5;241m.\u001b[39mcurrent_task():\n\u001b[0;32m      4\u001b[0m     Task\u001b[38;5;241m.\u001b[39mcurrent_task()\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m----> 6\u001b[0m task_lgbm \u001b[38;5;241m=\u001b[39m \u001b[43mTask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTemperature Forecasting\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# tên project trên ClearML (nếu chưa có sẽ tự tạo)\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRidge Optuna Tuning 1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# tên task mới\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTaskTypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# loại task (training / testing / optimizer ...)\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# print(\" Task created successfully!\")\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask ID:\u001b[39m\u001b[38;5;124m\"\u001b[39m, task_lgbm\u001b[38;5;241m.\u001b[39mid)\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\clearml\\task.py:640\u001b[0m, in \u001b[0;36mTask.init\u001b[1;34m(cls, project_name, task_name, task_type, tags, reuse_last_task_id, continue_last_task, output_uri, auto_connect_arg_parser, auto_connect_frameworks, auto_resource_monitoring, auto_connect_streams, deferred_init)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sub_process_task_id:\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 640\u001b[0m         task \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dev_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdefault_project_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdefault_task_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdefault_task_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreuse_last_task_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreuse_last_task_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontinue_last_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontinue_last_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdetect_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    649\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mauto_connect_frameworks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mauto_connect_frameworks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdetect_repository\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    654\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m            \u001b[49m\u001b[43mauto_connect_streams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_connect_streams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    657\u001b[0m         \u001b[38;5;66;03m# check if we are local rank 0 (local master),\u001b[39;00m\n\u001b[0;32m    658\u001b[0m         \u001b[38;5;66;03m# create an anchor with task ID for the other processes\u001b[39;00m\n\u001b[0;32m    659\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _local_rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\clearml\\task.py:4382\u001b[0m, in \u001b[0;36mTask._create_dev_task\u001b[1;34m(cls, default_project_name, default_task_name, default_task_type, tags, reuse_last_task_id, continue_last_task, detect_repo, auto_connect_streams)\u001b[0m\n\u001b[0;32m   4380\u001b[0m \u001b[38;5;66;03m# create a new task\u001b[39;00m\n\u001b[0;32m   4381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m default_task_id:\n\u001b[1;32m-> 4382\u001b[0m     task \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__create_protection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_project_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_task_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_task_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_to_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4389\u001b[0m     \u001b[38;5;66;03m# no need to reload yet, we clear this before the end of the function\u001b[39;00m\n\u001b[0;32m   4390\u001b[0m     task\u001b[38;5;241m.\u001b[39m_reload_skip_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\clearml\\task.py:246\u001b[0m, in \u001b[0;36mTask.__init__\u001b[1;34m(self, private, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UsageError(\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask object cannot be instantiated externally, use Task.current_task() or Task.get_task(...)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m     )\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_repo_detect_lock \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mRLock()\n\u001b[1;32m--> 246\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arguments \u001b[38;5;241m=\u001b[39m _Arguments(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\clearml\\backend_interface\\task\\task.py:199\u001b[0m, in \u001b[0;36mTask.__init__\u001b[1;34m(self, session, task_id, log, project_name, task_name, task_type, log_to_backend, raise_on_validation_errors, force_create)\u001b[0m\n\u001b[0;32m    197\u001b[0m task_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_task_id(task_id, log\u001b[38;5;241m=\u001b[39mlog) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force_create \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__edit_lock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_project_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\clearml\\backend_interface\\base.py:181\u001b[0m, in \u001b[0;36mIdObjectBase.__init__\u001b[1;34m(self, id, session, log, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mid\u001b[39m: \u001b[38;5;28mstr\u001b[39m, session: Session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, log: logging\u001b[38;5;241m.\u001b[39mLogger \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mIdObjectBase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\clearml\\backend_interface\\base.py:47\u001b[0m, in \u001b[0;36mInterfaceBase.__init__\u001b[1;34m(self, session, log, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, session: Session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, log: logging\u001b[38;5;241m.\u001b[39mLogger \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28msuper\u001b[39m(InterfaceBase, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session \u001b[38;5;241m=\u001b[39m session \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_default_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log \u001b[38;5;241m=\u001b[39m log \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_log()\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\clearml\\backend_interface\\base.py:152\u001b[0m, in \u001b[0;36mInterfaceBase._get_default_session\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_default_session\u001b[39m(\u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Session:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m InterfaceBase\u001b[38;5;241m.\u001b[39m_default_session:\n\u001b[1;32m--> 152\u001b[0m         InterfaceBase\u001b[38;5;241m.\u001b[39m_default_session \u001b[38;5;241m=\u001b[39m \u001b[43mSession\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m            \u001b[49m\u001b[43minitialize_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m            \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mENV_ACCESS_KEY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m            \u001b[49m\u001b[43msecret_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mENV_SECRET_KEY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m InterfaceBase\u001b[38;5;241m.\u001b[39m_default_session\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\clearml\\backend_api\\session\\session.py:177\u001b[0m, in \u001b[0;36mSession.__init__\u001b[1;34m(self, worker, api_key, secret_key, host, logger, verbose, config, http_retries_config, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mclearml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfigWrapper\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m ConfigWrapper\u001b[38;5;241m.\u001b[39m_init()\n\u001b[1;32m--> 177\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\clearml\\backend_api\\session\\session.py:241\u001b[0m, in \u001b[0;36mSession._connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__secret_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__init_secret_key \u001b[38;5;129;01mor\u001b[39;00m ENV_SECRET_KEY\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    237\u001b[0m         default\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi.credentials.secret_key\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_secret)\n\u001b[0;32m    238\u001b[0m     )\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msecret_key \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccess_key \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__auth_token:\n\u001b[1;32m--> 241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingConfigError()\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28msuper\u001b[39m(Session, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__token_manager_kwargs,\n\u001b[0;32m    245\u001b[0m     token_expiration_threshold_sec\u001b[38;5;241m=\u001b[39mtoken_expiration_threshold_sec,\n\u001b[0;32m    246\u001b[0m     req_token_expiration_sec\u001b[38;5;241m=\u001b[39mreq_token_expiration_sec\n\u001b[0;32m    247\u001b[0m )\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefresh_token()\n",
            "\u001b[1;31mMissingConfigError\u001b[0m: It seems ClearML is not configured on this machine!\nTo get started with ClearML, setup your own 'clearml-server' or create a free account at https://app.clear.ml\nSetup instructions can be found here: https://clear.ml/docs"
          ]
        }
      ],
      "source": [
        "# Nhớ đổi tên task trong task_name\n",
        "\n",
        "if Task.current_task():\n",
        "    Task.current_task().close()\n",
        "\n",
        "task_lgbm = Task.init(\n",
        "    project_name=\"Temperature Forecasting\",         # tên project trên ClearML (nếu chưa có sẽ tự tạo)\n",
        "    task_name=\"Ridge Optuna Tuning 1\",            # tên task mới\n",
        "    task_type=Task.TaskTypes.optimizer            # loại task (training / testing / optimizer ...)\n",
        ")\n",
        "\n",
        "# print(\" Task created successfully!\")\n",
        "print(\"Task ID:\", task_lgbm.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fb2d149",
      "metadata": {
        "id": "4fb2d149"
      },
      "outputs": [],
      "source": [
        "task_lgbm = Task.get_task(task_id=\"59847c902e2d4035b6d4bb584e1b3a78\")\n",
        "logger_lgbm = task_lgbm.get_logger()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5650ba77",
      "metadata": {
        "id": "5650ba77"
      },
      "source": [
        "### A. Chạy thử với các hyper parameter mặc định\n",
        "\n",
        "Before training the **Ridge Regression model**, several preprocessing classes were implemented to clean, encode, and transform the dataset.  \n",
        "These custom classes prepare the input data so that the pipeline can work efficiently and produce stable predictions.\n",
        "\n",
        "#### **1. `HandleMissing(drop_threshold = 0.05)`**\n",
        "\n",
        "**Purpose:**  \n",
        "Handle missing values (NaNs) in the dataset.\n",
        "\n",
        "**How it works:**  \n",
        "- If a column has more than **5% missing values**, it will be dropped.  \n",
        "- For remaining columns:  \n",
        "  - **Numeric features** → filled with **median**.  \n",
        "  - **Categorical features** → filled with **mode (most frequent value)**.\n",
        "\n",
        "**Why it’s needed:**  \n",
        "Weather datasets often have missing or corrupted records. Handling these ensures the model runs smoothly and avoids bias due to incomplete data.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. `DropLowVariance(threshold = 0.0)`**\n",
        "\n",
        "**Purpose:**  \n",
        "Remove features with very low variance (i.e., almost constant values).\n",
        "\n",
        "**How it works:**  \n",
        "- Keeps only columns whose variance > 0.0.  \n",
        "- Keeps non-numeric columns for later encoding.\n",
        "\n",
        "**Why it’s needed:**  \n",
        "Low-variance features (like “snow depth” or “snow presence”) add no predictive value and can introduce unnecessary noise.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. `DropCategorical(unique_ratio_threshold = 0.9)`**\n",
        "\n",
        "**Purpose:**  \n",
        "Drop categorical features that have too few or too many unique values.\n",
        "\n",
        "**How it works:**  \n",
        "- Removes columns that:\n",
        "  - Have **only one unique value**, or  \n",
        "  - Have **too high a uniqueness ratio** (>90%), which limits generalization.\n",
        "\n",
        "**Why it’s needed:**  \n",
        "Reduces data dimensionality and ensures only meaningful categorical features remain.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. `SeasonClassifier(temp_col='temp', n_seasons=5)`**\n",
        "\n",
        "**Purpose:**  \n",
        "Automatically classify months into seasons based on temperature distribution.\n",
        "\n",
        "**How it works:**  \n",
        "- Calculates **average monthly temperature**.  \n",
        "- Splits it into **five temperature-based season groups** using `qcut`.  \n",
        "- Assigns each month to its corresponding season.\n",
        "\n",
        "**Why it’s needed:**  \n",
        "Adding a season feature helps the model capture **cyclical weather patterns** (e.g., hot summers, cold winters).\n",
        "\n",
        "---\n",
        "\n",
        "### **5. `WindCategoryEncoder(wind_category_col='wind_category', n_quantiles=4)`**\n",
        "\n",
        "**Purpose:**  \n",
        "Encode wind category into numerical values based on its average temperature effect.\n",
        "\n",
        "**How it works:**  \n",
        "- Computes average temperature for each wind type.  \n",
        "- Sorts and groups them into **4 quantile-based categories**.  \n",
        "- Encodes each wind type with its group number.\n",
        "\n",
        "**Why it’s needed:**  \n",
        "Transforms qualitative wind types into quantitative values that the model can learn from.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. `ConditionsEncoder(encoding_method='target', n_quantiles=3)`**\n",
        "\n",
        "**Purpose:**  \n",
        "Encode the “weather conditions” (categorical) column into numeric form based on temperature or quantiles.\n",
        "\n",
        "**How it works:**  \n",
        "- `ordinal`: ranks based on average temperature.  \n",
        "- `target`: smooth target encoding using temperature means.  \n",
        "- `quantile`: divides into groups by temperature quantiles.\n",
        "\n",
        "**Why it’s needed:**  \n",
        "This converts a non-numeric but important variable into meaningful numeric information, preserving the relation between conditions and temperature.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. `To_Category()`**\n",
        "\n",
        "**Purpose:**  \n",
        "Convert object-type columns into `category` dtype to maintain consistency between training and test sets.\n",
        "\n",
        "**How it works:**  \n",
        "- Detects columns of type `object`.  \n",
        "- Stores valid category mappings from the training data.  \n",
        "- Ensures the same categories exist in the test set.\n",
        "\n",
        "**Why it’s needed:**  \n",
        "Prevents unseen category errors and keeps the dataset consistent for downstream models like Ridge or LightGBM.\n",
        "\n",
        "\n",
        "### **Conclusion:**  \n",
        "These preprocessing steps clean, normalize, and enrich the dataset before feeding it into the final Ridge pipeline (`DropHighlyCorrelated1 → StandardScaler → Ridge`).  \n",
        "They ensure the data is high-quality, consistent, and ready for accurate weather forecasting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43500a96",
      "metadata": {
        "id": "43500a96"
      },
      "outputs": [],
      "source": [
        "# === 1️ Cấu hình Ridge mặc định ===\n",
        "default_model_params = dict(\n",
        "    alpha=1.0,\n",
        "    fit_intercept=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# === 2️ Cấu hình Feature Selection ===\n",
        "fs_params = dict(\n",
        "    corr_threshold=0.8,  # Cần tuning threshold\n",
        ")\n",
        "\n",
        "# === 3️ Pipeline gồm feature selection + scaler + Ridge ===\n",
        "ridge_pipeline = Pipeline([\n",
        "    (\"feature_selection\", fs.DropHighlyCorrelated1(threshold=fs_params['corr_threshold'])),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"ridge\", Ridge(**default_model_params))\n",
        "])\n",
        "\n",
        "# === 4 Time Series Cross Validation ===\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "cv_scores = []\n",
        "cv_artifacts = {}\n",
        "\n",
        "print(\"=== Time Series Cross-Validation (Ridge + Scaling) ===\")\n",
        "\n",
        "fold_idx = 1\n",
        "for train_idx, val_idx in tscv.split(X_train):\n",
        "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "    # Huấn luyện riêng cho từng target\n",
        "    y_pred_val_all = pd.DataFrame(index=y_val.index, columns=y_train.columns)\n",
        "    for target_col in y_train.columns:\n",
        "        model = ridge_pipeline\n",
        "        model.fit(X_tr, y_tr[target_col])\n",
        "        y_pred_val_all[target_col] = model.predict(X_val)\n",
        "\n",
        "    # Đánh giá\n",
        "    metrics_val = evaluate_multi_output(y_val, y_pred_val_all)\n",
        "    avg_val = metrics_val[\"average\"]\n",
        "    per_day_val = metrics_val[\"per_day\"]\n",
        "    cv_scores.append(avg_val)\n",
        "\n",
        "    print(f\"\\nFold {fold_idx} validation metrics:\")\n",
        "    print(avg_val)\n",
        "    print(per_day_val)\n",
        "\n",
        "    # Log artifact thay vì scalar\n",
        "    cv_artifacts[f\"Fold_{fold_idx}\"] = {\n",
        "        \"average\": avg_val,\n",
        "        \"per_day\": per_day_val\n",
        "    }\n",
        "\n",
        "    fold_idx += 1\n",
        "\n",
        "\n",
        "# === 5 Tổng hợp kết quả cross-validation ===\n",
        "cv_df = pd.DataFrame(cv_scores)\n",
        "cv_mean = cv_df.mean()\n",
        "cv_std = cv_df.std()\n",
        "\n",
        "print(\"\\n=====> Cross-validation trung bình:\")\n",
        "print(cv_mean)\n",
        "print(\"\\n=====> Cross-validation độ lệch chuẩn:\")\n",
        "print(cv_std)\n",
        "\n",
        "# Log artifact tổng hợp CV (mean + std)\n",
        "cv_artifacts[\"CV_Summary\"] = {\n",
        "    \"mean\": cv_mean.to_dict(),\n",
        "    \"std\": cv_std.to_dict()\n",
        "}\n",
        "\n",
        "# Gửi toàn bộ artifact CV lên ClearML\n",
        "task_lgbm.upload_artifact(\"Default_Ridge_CV_Detail\", cv_artifacts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d951ebc2",
      "metadata": {
        "id": "d951ebc2"
      },
      "source": [
        "### B. Tuning với Scaler, Drop Highly Correlated threshold, Model Ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6a9c92d1",
      "metadata": {
        "id": "6a9c92d1"
      },
      "outputs": [],
      "source": [
        "## === A. Optuna Objective cho từng target RIÊNG BIỆT ==\n",
        "def objective_ridge_per_target(trial, target_name, target_idx):\n",
        "   # Chọn scaler\n",
        "   scaler_name = trial.suggest_categorical(\"scaler\", [\"standard\", \"minmax\", \"robust\"])\n",
        "   if scaler_name == \"standard\":\n",
        "      scaler = StandardScaler()\n",
        "   elif scaler_name == \"minmax\":\n",
        "      scaler = MinMaxScaler()\n",
        "   else:\n",
        "      scaler = RobustScaler()\n",
        "\n",
        "   # Hyperparameter cho Ridge\n",
        "   alpha = trial.suggest_categorical(\"alpha\", [0.001, 0.01, 0.1, 1.0, 10.0, 50.0, 100.0])\n",
        "   fit_intercept = trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
        "\n",
        "   # Hyperparameter cho Feature Selection - THÊM VÀO\n",
        "   corr_threshold = trial.suggest_float(\"corr_threshold\", 0.7, 0.95, step=0.05)\n",
        "\n",
        "   # TimeSeriesSplit\n",
        "   cv = TimeSeriesSplit(n_splits=5)\n",
        "   rmse_scores = []\n",
        "\n",
        "   for train_idx, val_idx in cv.split(X_train):\n",
        "      X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "      y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "      # Áp dụng feature selection cho từng fold - THÊM VÀO\n",
        "      high = fs.DropHighlyCorrelated1(threshold=corr_threshold)\n",
        "      X_tr_fs = high.fit_transform(X_tr)\n",
        "      X_val_fs = high.transform(X_val)\n",
        "\n",
        "      # Chỉ lấy target hiện tại\n",
        "      y_tr_single = y_tr.iloc[:, target_idx] if len(y_tr.shape) > 1 else y_tr\n",
        "      y_val_single = y_val.iloc[:, target_idx] if len(y_val.shape) > 1 else y_val\n",
        "\n",
        "      # Pipeline: scaler + Ridge (Single output)\n",
        "      ridge = Ridge(\n",
        "         alpha=alpha,\n",
        "         fit_intercept=fit_intercept,\n",
        "         random_state=42\n",
        "      )\n",
        "      pipeline = Pipeline([\n",
        "         (\"scaler\", scaler),\n",
        "         (\"model\", ridge)\n",
        "      ])\n",
        "\n",
        "      pipeline.fit(X_tr_fs, y_tr_single)\n",
        "      y_pred_val = pipeline.predict(X_val_fs)\n",
        "\n",
        "      # Tính RMSE cho target hiện tại\n",
        "      result = evaluate(y_val_single, y_pred_val)\n",
        "      rmse = result['RMSE']\n",
        "      rmse_scores.append(rmse)\n",
        "\n",
        "   mean_rmse = np.mean(rmse_scores)\n",
        "\n",
        "   # Log lên ClearML với series theo target\n",
        "   logger_lgbm.report_scalar(\n",
        "      title=f'Optuna Tuning - Ridge [{target_name}]',\n",
        "      series='Trial RMSE',\n",
        "      value=mean_rmse,\n",
        "      iteration=trial.number\n",
        "   )\n",
        "\n",
        "   params_table = pd.DataFrame([{\n",
        "      \"target\": target_name,\n",
        "      \"scaler\": scaler_name,\n",
        "      \"alpha\": alpha,\n",
        "      \"fit_intercept\": fit_intercept,\n",
        "      \"corr_threshold\": corr_threshold,\n",
        "   }])\n",
        "\n",
        "   logger_lgbm.report_table(\n",
        "      title=f\"Trial {trial.number} - {target_name}\",\n",
        "      series=\"params\",\n",
        "      iteration=trial.number,\n",
        "      table_plot=params_table\n",
        "   )\n",
        "\n",
        "   return mean_rmse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9bfc3731",
      "metadata": {
        "id": "9bfc3731"
      },
      "outputs": [],
      "source": [
        "## === B. Hàm helper ===\n",
        "def get_scaler_from_params(scaler_name):\n",
        "   if scaler_name == \"standard\":\n",
        "      return StandardScaler()\n",
        "   elif scaler_name == \"minmax\":\n",
        "      return MinMaxScaler()\n",
        "   else:\n",
        "      return RobustScaler()\n",
        "\n",
        "def extract_ridge_params(params):\n",
        "   return {\n",
        "      'alpha': params['alpha'],\n",
        "      'fit_intercept': params['fit_intercept'],\n",
        "      'random_state': 42\n",
        "   }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5cda1294",
      "metadata": {
        "id": "5cda1294"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-10 00:37:47,757] A new study created in memory with name: no-name-b5c721cf-d62d-4afe-bf81-4ae1d38dd3bb\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning Ridge for target: temp_next_1 (1/5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[W 2025-11-10 00:37:50,021] Trial 0 failed with parameters: {'scaler': 'minmax', 'alpha': 10.0, 'fit_intercept': False, 'corr_threshold': 0.8999999999999999} because of the following error: NameError(\"name 'logger_lgbm' is not defined\").\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_12300\\695326086.py\", line 22, in <lambda>\n",
            "    lambda trial: objective_ridge_per_target(trial, target_name, idx),\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_12300\\977673883.py\", line 58, in objective_ridge_per_target\n",
            "    logger_lgbm.report_scalar(\n",
            "    ^^^^^^^^^^^\n",
            "NameError: name 'logger_lgbm' is not defined\n",
            "[W 2025-11-10 00:37:50,024] Trial 0 failed with value None.\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'logger_lgbm' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 21\u001b[0m\n\u001b[0;32m     15\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[0;32m     16\u001b[0m    direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     17\u001b[0m    sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Gọi Study Optimize\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective_ridge_per_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m   \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m   \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Lưu best params và model cho target này\u001b[39;00m\n\u001b[0;32m     28\u001b[0m best_params_per_target[target_name] \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    390\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 490\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial_id \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:258\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    254\u001b[0m     updated_state \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    257\u001b[0m ):\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trial\u001b[38;5;241m.\u001b[39m_trial_id\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
            "Cell \u001b[1;32mIn[10], line 22\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     15\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[0;32m     16\u001b[0m    direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     17\u001b[0m    sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Gọi Study Optimize\u001b[39;00m\n\u001b[0;32m     21\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\n\u001b[1;32m---> 22\u001b[0m    \u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective_ridge_per_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     23\u001b[0m    n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     24\u001b[0m    show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Lưu best params và model cho target này\u001b[39;00m\n\u001b[0;32m     28\u001b[0m best_params_per_target[target_name] \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams\n",
            "Cell \u001b[1;32mIn[8], line 58\u001b[0m, in \u001b[0;36mobjective_ridge_per_target\u001b[1;34m(trial, target_name, target_idx)\u001b[0m\n\u001b[0;32m     55\u001b[0m mean_rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(rmse_scores)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Log lên ClearML với series theo target\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m \u001b[43mlogger_lgbm\u001b[49m\u001b[38;5;241m.\u001b[39mreport_scalar(\n\u001b[0;32m     59\u001b[0m    title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOptuna Tuning - Ridge [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     60\u001b[0m    series\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrial RMSE\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     61\u001b[0m    value\u001b[38;5;241m=\u001b[39mmean_rmse,\n\u001b[0;32m     62\u001b[0m    iteration\u001b[38;5;241m=\u001b[39mtrial\u001b[38;5;241m.\u001b[39mnumber\n\u001b[0;32m     63\u001b[0m )\n\u001b[0;32m     65\u001b[0m params_table \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([{\n\u001b[0;32m     66\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target_name,\n\u001b[0;32m     67\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m\"\u001b[39m: scaler_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorr_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m: corr_threshold,\n\u001b[0;32m     71\u001b[0m }])\n\u001b[0;32m     73\u001b[0m logger_lgbm\u001b[38;5;241m.\u001b[39mreport_table(\n\u001b[0;32m     74\u001b[0m    title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial\u001b[38;5;241m.\u001b[39mnumber\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     75\u001b[0m    series\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     76\u001b[0m    iteration\u001b[38;5;241m=\u001b[39mtrial\u001b[38;5;241m.\u001b[39mnumber,\n\u001b[0;32m     77\u001b[0m    table_plot\u001b[38;5;241m=\u001b[39mparams_table\n\u001b[0;32m     78\u001b[0m )\n",
            "\u001b[1;31mNameError\u001b[0m: name 'logger_lgbm' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "## === C.. Tune riêng cho từng target ===\n",
        "best_models_per_target = {}\n",
        "best_params_per_target = {}\n",
        "feature_selectors_per_target = {}\n",
        "\n",
        "\n",
        "# Tắt logging Nó hiện nhiều quá thì bỏ comment dòng này ra, đằng nào cũng quan sát được trên clearml\n",
        "# optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "\n",
        "for idx, target_name in enumerate(y_train.columns):\n",
        "   print(f\"Tuning Ridge for target: {target_name} ({idx + 1}/{len(y_train.columns)})\")\n",
        "\n",
        "   # Tạo study riêng cho mỗi target\n",
        "   study = optuna.create_study(\n",
        "      direction='minimize',\n",
        "      sampler=optuna.samplers.TPESampler(seed=42)\n",
        "   )\n",
        "\n",
        "   # Gọi Study Optimize\n",
        "   study.optimize(\n",
        "      lambda trial: objective_ridge_per_target(trial, target_name, idx),\n",
        "      n_trials=100,\n",
        "      show_progress_bar=False\n",
        "   )\n",
        "\n",
        "   # Lưu best params và model cho target này\n",
        "   best_params_per_target[target_name] = study.best_trial.params\n",
        "   best_params_per_target[target_name]['target_idx'] = idx\n",
        "\n",
        "   # Fit best model cho target này với feature selection\n",
        "   best_scaler = get_scaler_from_params(study.best_trial.params[\"scaler\"])\n",
        "   best_ridge = Ridge(**extract_ridge_params(study.best_trial.params))\n",
        "\n",
        "   # Áp dụng feature selection trên toàn bộ training data\n",
        "   high = fs.DropHighlyCorrelated1(threshold=study.best_trial.params[\"corr_threshold\"])\n",
        "   X_train_fs = high.fit_transform(X_train)\n",
        "\n",
        "   # Lưu feature selectors để sử dụng sau\n",
        "   feature_selectors_per_target[target_name] = {\n",
        "      'correlation_selector': high,\n",
        "   }\n",
        "\n",
        "   best_pipeline = Pipeline([\n",
        "      (\"scaler\", best_scaler),\n",
        "      (\"model\", best_ridge)\n",
        "   ])\n",
        "\n",
        "   # Train trên toàn bộ data đã được feature selection\n",
        "   y_target = y_train.iloc[:, idx] if len(y_train.shape) > 1 else y_train\n",
        "   best_pipeline.fit(X_train_fs, y_target)\n",
        "   best_models_per_target[target_name] = best_pipeline\n",
        "\n",
        "   # Log kết quả từng target lên ClearML\n",
        "   logger_lgbm.report_scalar(\n",
        "      title='Best RMSE per Target',\n",
        "      series=target_name,\n",
        "      value=study.best_value,\n",
        "      iteration=idx\n",
        "   )\n",
        "\n",
        "   print(f\" {target_name} - Best RMSE: {study.best_value:.4f}\")\n",
        "   print(f\" {target_name} - Final features: {X_train_fs.shape[1]}\")\n",
        "\n",
        "# Khôi phục logging\n",
        "#optuna.logging.set_verbosity(optuna.logging.INFO)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02d2a84a",
      "metadata": {
        "id": "02d2a84a"
      },
      "source": [
        "### C. Training final model với bộ hyper paramter và đánh giá cuối cùng trên Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "58dcd372",
      "metadata": {
        "id": "58dcd372"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "need at least one array to concatenate",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m final_multi_model \u001b[38;5;241m=\u001b[39m CustomMultiOutputRegressor(best_models_per_target, feature_selectors_per_target)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m## === E. Đánh giá final model ===\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m y_pred_test \u001b[38;5;241m=\u001b[39m \u001b[43mfinal_multi_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m y_pred_train \u001b[38;5;241m=\u001b[39m final_multi_model\u001b[38;5;241m.\u001b[39mpredict(X_train)\n\u001b[0;32m     32\u001b[0m final_test_metrics \u001b[38;5;241m=\u001b[39m evaluate_multi_output(y_test, y_pred_test)\n",
            "Cell \u001b[1;32mIn[11], line 17\u001b[0m, in \u001b[0;36mCustomMultiOutputRegressor.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     15\u001b[0m    pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_fs)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m    predictions\u001b[38;5;241m.\u001b[39mappend(pred)\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\shape_base.py:359\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mValueError\u001b[0m: need at least one array to concatenate"
          ]
        }
      ],
      "source": [
        "## === D. Tạo MultiOutput wrapper từ các model riêng ===\n",
        "class CustomMultiOutputRegressor:\n",
        "   def __init__(self, models_per_target, feature_selectors_per_target):\n",
        "      self.models_per_target = models_per_target\n",
        "      self.feature_selectors_per_target = feature_selectors_per_target\n",
        "      self.target_names = list(models_per_target.keys())\n",
        "\n",
        "   def predict(self, X):\n",
        "      predictions = []\n",
        "      for target_name, model in self.models_per_target.items():\n",
        "         # Áp dụng feature selection tương ứng cho từng target\n",
        "         selectors = self.feature_selectors_per_target[target_name]\n",
        "         X_fs = selectors['correlation_selector'].transform(X)\n",
        "\n",
        "         pred = model.predict(X_fs).reshape(-1, 1)\n",
        "         predictions.append(pred)\n",
        "      return np.hstack(predictions)\n",
        "\n",
        "   def get_params(self, deep=True):\n",
        "      return {\n",
        "         \"models_per_target\": self.models_per_target,\n",
        "         \"feature_selectors_per_target\": self.feature_selectors_per_target\n",
        "      }\n",
        "\n",
        "# Tạo final model với feature selectors\n",
        "final_multi_model = CustomMultiOutputRegressor(best_models_per_target, feature_selectors_per_target)\n",
        "\n",
        "## === E. Đánh giá final model ===\n",
        "y_pred_test = final_multi_model.predict(X_test)\n",
        "y_pred_train = final_multi_model.predict(X_train)\n",
        "\n",
        "final_test_metrics = evaluate_multi_output(y_test, y_pred_test)\n",
        "final_train_metrics = evaluate_multi_output(y_train, y_pred_train)\n",
        "\n",
        "print(\"Final Model - Test metrics (average):\", final_test_metrics[\"average\"])\n",
        "print(\"Final Model - Test metrics (perday):\", final_test_metrics[\"per_day\"])\n",
        "print()\n",
        "print(\"Final Model - Train metrics (average):\", final_train_metrics[\"average\"])\n",
        "print(\"Final Model - Train metrics (perday):\", final_train_metrics[\"per_day\"])\n",
        "\n",
        "# Log final results\n",
        "task_lgbm.upload_artifact(\"Best Parameters Per Target\", best_params_per_target)\n",
        "task_lgbm.upload_artifact(\"Feature Selectors Per Target\", feature_selectors_per_target)\n",
        "task_lgbm.upload_artifact(\"Final Model - Test Metrics\", final_test_metrics)\n",
        "task_lgbm.upload_artifact(\"Final Model - Train Metrics\", final_train_metrics)\n",
        "\n",
        "## === F. Lưu tất cả models ===\n",
        "ridge_models_path = r\"models/Ridge_models_per_target.pkl\"\n",
        "joblib.dump({\n",
        "   'models': best_models_per_target,\n",
        "   'params': best_params_per_target,\n",
        "   'feature_selectors': feature_selectors_per_target,\n",
        "   'final_model': final_multi_model\n",
        "}, ridge_models_path)\n",
        "\n",
        "print('\\n')\n",
        "print(\"=== FEATURE SELECTION SUMMARY ===\")\n",
        "for target_name in y_train.columns:\n",
        "    selectors = feature_selectors_per_target[target_name]\n",
        "    original_features = X_train.shape[1]\n",
        "\n",
        "    # Chỉ còn correlation_selector, không còn feature_selector nữa\n",
        "    correlation_selector = selectors['correlation_selector']\n",
        "\n",
        "    # Tính số features sau khi drop correlated\n",
        "    features_after_corr = original_features - len(correlation_selector.to_drop_)\n",
        "\n",
        "    print(f\"{target_name}: {original_features} → {features_after_corr} features\")\n",
        "    print(f\"  - Dropped {len(correlation_selector.to_drop_)} highly correlated features\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0038d83f",
      "metadata": {
        "id": "0038d83f"
      },
      "source": [
        "### Thêm link log kết quả tổng hợp của ClearML vào đây, kèm mấy cái ảnh biểu đồ tuning qua các trial, giải thích quá trình tune (mấy cái biểu đồ đậm nhạt, các thứ ấy để thầy biết là  mình có tune tử tế tinh chỉnh hyper các kiểu cho nó hiệu quả hơn) - Nhàn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb0a9a8c",
      "metadata": {
        "id": "cb0a9a8c"
      },
      "source": [
        "### 2. LGB Model (Our Final Model) - Sương Mai, Hoàng\n",
        "- Viết giải thích ngắn, tại sao chọn Model điểm ưu việt của nó, ... có nhiều bài trên mạng nó nói geekforgeek, ... lên đấy cop rồi tự điều chỉnh lại\n",
        "\n",
        "- Các phần so sánh thì chưa cần tune, bao giờ so sánh xong, chọn được option tốt nhất thì mình sẽ tune cái option tốt nhất đấy nhé"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce1ed18f",
      "metadata": {
        "id": "ce1ed18f"
      },
      "source": [
        "### 2.1. Data preparation for LGB\n",
        "- Sẽ loại bỏ những biến interaction (giải thích vì sao, vdu như do tree tự học được): is_linear = False (Khi chạy thử thì có thêm vào kết quả tệ đi thật)\n",
        "- Các biến category có 2 lựa chọn là để nguyên hoặc encoding\n",
        "- Drop các base feature\n",
        "\n",
        "    base = ['tempmax', 'tempmin', 'temp', 'feelslikemax', 'feelslikemin',\n",
        "       'feelslike', 'dew', 'humidity', 'precip', 'precipprob', 'precipcover',\n",
        "       'preciptype', 'snow', 'snowdepth', 'windgust', 'windspeed', 'winddir',\n",
        "       'sealevelpressure', 'cloudcover', 'visibility', 'solarradiation',\n",
        "       'solarenergy', 'uvindex', 'severerisk', 'moonphase']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e03c84ce",
      "metadata": {
        "id": "e03c84ce"
      },
      "outputs": [],
      "source": [
        "# 1. Không sử dụng các biến interaction: is_linear = False\n",
        "\n",
        "# Tạo feature engineering (drop NaN sau khi rolling/lag)\n",
        "train_feat, target_col = fe.feature_engineering(train_processed, is_drop_nan= True, is_linear= False) # không tạo biến interaction\n",
        "test_feat, _ = fe.feature_engineering(test_processed, is_drop_nan= True, is_linear= False)\n",
        "\n",
        "# Chia X, y riêng biệt\n",
        "X_train = train_feat.drop(columns= target_col)\n",
        "y_train = train_feat[target_col]\n",
        "\n",
        "X_test = test_feat.drop(columns= target_col)\n",
        "y_test = test_feat[target_col]\n",
        "\n",
        "# 2. Drop base - Chỉ sử dụng derive feature\n",
        "print('\\nDrop Base')\n",
        "print('số lượng Trước Khi drop base', X_train.shape, X_test.shape)\n",
        "X_train = fe.drop_base_features(X_train)\n",
        "X_test = fe.drop_base_features(X_test)\n",
        "print('số lượng sau drop base', X_train.shape, X_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8347e7f",
      "metadata": {
        "id": "b8347e7f"
      },
      "source": [
        "### 2.2. Category Feature"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e40d7ec5",
      "metadata": {
        "id": "e40d7ec5"
      },
      "source": [
        "#### A. Các biến category trong Input của mình - Sương Mai\n",
        "\n",
        "- Ngoài 'conditions' có sẵn,  thêm 2 category nữa là 'wind_category' và 'season'\n",
        "- Pipeline sử dụng để encode `pl.build_encoding_pipeline(is_category= True)` trong file `pipeline.py`\n",
        "      ![image.png](attachment:image.png)\n",
        "\n",
        "- Thêm bảng giải thích ngắn gọn 3 kiểu encoding: target encoding, ordinal encoding, quantile\n",
        "- (Có thể tạo bảng, nêu ngắn gọn mỗi biến có các unique value nào, ... thấy gì phù hợp thì thêm vào)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f4d9478",
      "metadata": {
        "id": "4f4d9478"
      },
      "outputs": [],
      "source": [
        "# 3. Kiểm tra các biến Category\n",
        "encoder = pl.build_encoding_pipeline(is_category=True)\n",
        "encoder.fit(X_train, train_feat['temp'])\n",
        "\n",
        "X_train = encoder.transform(X_train)\n",
        "X_test = encoder.transform(X_test)\n",
        "\n",
        "# Kích thước dữ liệu\n",
        "print(\"=\" * 60)\n",
        "print(f\" Kích thước dữ liệu:\")\n",
        "print(f\"   - Train: {X_train.shape} | Target: {y_train.shape}\")\n",
        "print(f\"   - Test : {X_test.shape} | Target: {y_test.shape}\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "cat_features = X_train.select_dtypes(include=['category']).columns\n",
        "\n",
        "print(\"\\nThông tin chi tiết các feature categorical:\")\n",
        "for col in cat_features:\n",
        "    unique_vals = list(X_train[col].unique())\n",
        "    print(f\"• {col} ({len(unique_vals)} lớp): {unique_vals}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc0fa846",
      "metadata": {
        "id": "fc0fa846"
      },
      "source": [
        "### B. Nói ngắn gọn cách/ thuật toán LGB xử lí Category - Sương Mai\n",
        "Sử dụng thuật toán gì, ....\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b76580a7",
      "metadata": {
        "id": "b76580a7"
      },
      "source": [
        "### 2.3. Tuning and Training LGB\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b89415c8",
      "metadata": {
        "id": "b89415c8"
      },
      "source": [
        "### A. Giải thích các hyper paramter mình tune - Sương Mai, Hoàng đọc tìm hiểu luôn\n",
        "vd:\n",
        "\n",
        "Trong cái boosting type (check lại kiến thức t vt nhé ko đảm bảo)\n",
        "<br> goss: lựa chọn ngẫu nhiên dựa theo độ lớn gradient, tập trung vào các point có grad lớn - thì không dùng được subsample và subsample_fraction\n",
        "<br> gbdt, dart: lựa chọn ngẫu nhiên dựa vào bagging (chọn ngẫu nhiên qua tỉ lệ subsample và subsample_fraction)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc8ef6bb",
      "metadata": {
        "id": "fc8ef6bb"
      },
      "source": [
        "### B. Sử dụng hyper parameter mặc định của model và sử dụng category feature chưa encoding sang numeric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61c20978",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "61c20978",
        "outputId": "2d4f3930-0ec3-4f86-9d19-1fcfa73d54a4"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Task' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1144279217.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Đổi tên task trong task_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mTask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mTask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m task_lgbm = Task.init(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Task' is not defined"
          ]
        }
      ],
      "source": [
        "# Đổi tên task trong task_name\n",
        "if Task.current_task():\n",
        "    Task.current_task().close()\n",
        "\n",
        "task_lgbm = Task.init(\n",
        "    project_name=\"Temperature Forecasting\",         # tên project trên ClearML (nếu chưa có sẽ tự tạo)\n",
        "    task_name=\"LGB Optuna Tuning 1\",            # tên task mới\n",
        "    task_type=Task.TaskTypes.optimizer            # loại task (training / testing / optimizer ...)\n",
        ")\n",
        "\n",
        "# print(\" Task created successfully!\")\n",
        "print(\"Task ID:\", task_lgbm.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59f2cda7",
      "metadata": {
        "id": "59f2cda7"
      },
      "outputs": [],
      "source": [
        "task_lgbm = Task.get_task(task_id=\"648bdd16f71f4f80a010f6ae7ec29ba1\")\n",
        "logger_lgbm = task_lgbm.get_logger()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24bd6e35",
      "metadata": {
        "id": "24bd6e35"
      },
      "outputs": [],
      "source": [
        "# === 1️. Cấu hình LGBM mặc định ===\n",
        "default_lgbm_params = dict(\n",
        "    boosting_type='gbdt',\n",
        "    objective='regression',\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=-1,  # -1 means no limit\n",
        "    num_leaves=31,\n",
        "    min_child_samples=20,\n",
        "    subsample=1.0,\n",
        "    colsample_bytree=1.0,\n",
        "    reg_alpha=0.0,\n",
        "    reg_lambda=0.0,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "# ===2. Pipeline chỉ gồm encoding + LGBM ===\n",
        "lgbm_pipeline = Pipeline([\n",
        "    (\"encoding\", pl.build_encoding_pipeline(\n",
        "        is_category=False,\n",
        "        encoding_method_condition='target',\n",
        "        n_seasons=5,\n",
        "        n_quantiles=4\n",
        "    )),\n",
        "    (\"lgbm\", LGBMRegressor(**default_lgbm_params))\n",
        "])\n",
        "\n",
        "# === 3. Time Series Cross Validation ===\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "cv_scores = []\n",
        "cv_artifacts = {}\n",
        "\n",
        "print(\"=== Time Series Cross-Validation (LGBM + Encoding) ===\")\n",
        "\n",
        "fold_idx = 1\n",
        "for train_idx, val_idx in tscv.split(X_train):\n",
        "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "    # Huấn luyện riêng cho từng target\n",
        "    y_pred_val_all = pd.DataFrame(index=y_val.index, columns=y_train.columns)\n",
        "\n",
        "    # Theo dõi số lượng features\n",
        "    feature_counts = {}\n",
        "\n",
        "    for target_col in y_train.columns:\n",
        "        # Clone pipeline để tránh ảnh hưởng giữa các target\n",
        "        model = clone(lgbm_pipeline)\n",
        "\n",
        "        # Fit model\n",
        "        model.fit(X_tr, y_tr[target_col])\n",
        "        y_pred_val_all[target_col] = model.predict(X_val)\n",
        "\n",
        "        # Lấy thông tin encoding\n",
        "        X_encoded = model.named_steps['encoding'].transform(X_tr.head(1))\n",
        "        feature_counts[target_col] = {\n",
        "            'original_features': X_tr.shape[1],\n",
        "            'final_features': X_encoded.shape[1]\n",
        "        }\n",
        "\n",
        "    # Đánh giá\n",
        "    metrics_val = evaluate_multi_output(y_val, y_pred_val_all)\n",
        "    avg_val = metrics_val[\"average\"]\n",
        "    per_day_val = metrics_val[\"per_day\"]\n",
        "    cv_scores.append(avg_val)\n",
        "\n",
        "    print()\n",
        "    print(\"Metrics:\")\n",
        "    print(avg_val)\n",
        "    print(\"Per day metrics:\")\n",
        "    print(per_day_val)\n",
        "    print()\n",
        "\n",
        "    # Log artifact\n",
        "    cv_artifacts[f\"Fold_{fold_idx}\"] = {\n",
        "        \"average\": avg_val,\n",
        "        \"per_day\": per_day_val,\n",
        "        \"feature_counts\": feature_counts\n",
        "    }\n",
        "\n",
        "    fold_idx += 1\n",
        "\n",
        "# === 4. Tổng hợp kết quả cross-validation ===\n",
        "cv_df = pd.DataFrame(cv_scores)\n",
        "cv_mean = cv_df.mean()\n",
        "cv_std = cv_df.std()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"=====> Cross-validation trung bình:\")\n",
        "print(cv_mean)\n",
        "print(\"\\n=====> Cross-validation độ lệch chuẩn:\")\n",
        "print(cv_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "865cf620",
      "metadata": {
        "id": "865cf620"
      },
      "outputs": [],
      "source": [
        "# Log artifact tổng hợp CV (mean + std)\n",
        "cv_artifacts[\"CV_Summary\"] = {\n",
        "    \"mean\": cv_mean.to_dict(),\n",
        "    \"std\": cv_std.to_dict(),\n",
        "    \"model_params\": default_lgbm_params,\n",
        "    \"encoding_params\": {\n",
        "        \"is_category\": False,\n",
        "        \"encoding_method_condition\": \"target\",\n",
        "        \"n_seasons\": 5,\n",
        "        \"n_quantiles\": 4\n",
        "    }\n",
        "}\n",
        "\n",
        "# Gửi toàn bộ artifact CV lên ClearML\n",
        "task_lgbm.upload_artifact(\"Default_LGBM_CV_Detail\", cv_artifacts)\n",
        "\n",
        "print(\"\\n=== DEFAULT LGBM MODEL CONFIGURATION ===\")\n",
        "print(f\"Boosting type: {default_lgbm_params['boosting_type']}\")\n",
        "print(f\"Learning rate: {default_lgbm_params['learning_rate']}\")\n",
        "print(f\"Number of estimators: {default_lgbm_params['n_estimators']}\")\n",
        "print(f\"Max depth: {default_lgbm_params['max_depth']}\")\n",
        "print(f\"Number of leaves: {default_lgbm_params['num_leaves']}\")\n",
        "print(f\"Encoding method: target\")\n",
        "print(f\"Number of seasons: 5\")\n",
        "print(f\"Number of quantiles: 4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1260c3da",
      "metadata": {
        "id": "1260c3da"
      },
      "source": [
        "### B. Tuning Các lựa chọn encoding + Model LGB"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a04699c3",
      "metadata": {
        "id": "a04699c3"
      },
      "source": [
        "chạy lâu quá, nên t chạy thử đén trial 35 của temp next 1 thôi, c xem cách nào cải thiện ko thì chịu khó :'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c19a2dcf",
      "metadata": {
        "id": "c19a2dcf"
      },
      "outputs": [],
      "source": [
        "# === 1. Objective function cho LGBM ===\n",
        "def objective_lgbm_per_target(trial, target_name, target_idx):\n",
        "    # Hyperparameters cho encoding pipeline\n",
        "    encoding_method_condition = trial.suggest_categorical(\"encoding_method_condition\",\n",
        "                                                         [\"ordinal\", \"target\", \"quantile\"])\n",
        "    n_seasons = trial.suggest_int(\"n_seasons\", 3, 8)\n",
        "    n_quantiles = trial.suggest_int(\"n_quantiles\", 2, 6)\n",
        "\n",
        "    # Tune is_category cho từng encoder\n",
        "    conditions_is_category = trial.suggest_categorical(\"conditions_is_category\", [True, False])\n",
        "    season_is_category = trial.suggest_categorical(\"season_is_category\", [True, False])\n",
        "    wind_is_category = trial.suggest_categorical(\"wind_is_category\", [True, False])\n",
        "\n",
        "    # Hyperparameters cho LGBM\n",
        "    boosting_type = trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"dart\", \"goss\"])\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True)\n",
        "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000)\n",
        "    max_depth = trial.suggest_int(\"max_depth\", 3, 12)\n",
        "    num_leaves = trial.suggest_int(\"num_leaves\", 15, 255)\n",
        "    min_child_samples = trial.suggest_int(\"min_child_samples\", 5, 100)\n",
        "    min_split_gain = trial.suggest_float(\"min_split_gain\", 0.0, 1.0)\n",
        "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0)\n",
        "    reg_alpha = trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True)\n",
        "    reg_lambda = trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True)\n",
        "\n",
        "    # Conditional parameters\n",
        "    if boosting_type in ['goss', 'dart']:\n",
        "        subsample = trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
        "    else:\n",
        "        subsample = 1.0\n",
        "\n",
        "    if boosting_type == 'dart':\n",
        "        subsample_freq = trial.suggest_int(\"subsample_freq\", 1, 10)\n",
        "    else:\n",
        "        subsample_freq = 0\n",
        "\n",
        "    # TimeSeriesSplit\n",
        "    cv = TimeSeriesSplit(n_splits=5)\n",
        "    rmse_scores = []\n",
        "\n",
        "    for train_idx, val_idx in cv.split(X_train):\n",
        "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        # Chỉ lấy target hiện tại\n",
        "        y_tr_single = y_tr.iloc[:, target_idx] if len(y_tr.shape) > 1 else y_tr\n",
        "        y_val_single = y_val.iloc[:, target_idx] if len(y_val.shape) > 1 else y_val\n",
        "\n",
        "        try:\n",
        "            # Build encoding pipeline với các parameters được tuning\n",
        "            encoding_pipeline = pl.build_encoding_pipeline(\n",
        "                is_category=conditions_is_category,\n",
        "                encoding_method_condition=encoding_method_condition,\n",
        "                n_seasons=n_seasons,\n",
        "                n_quantiles=n_quantiles\n",
        "            )\n",
        "\n",
        "            # Override is_category cho từng component\n",
        "            encoding_pipeline.named_steps['season_encode'].is_category = season_is_category\n",
        "            encoding_pipeline.named_steps['wind_encode'].is_category = wind_is_category\n",
        "\n",
        "            # Apply encoding\n",
        "            X_tr_encoded = encoding_pipeline.fit_transform(X_tr, y_tr_single)\n",
        "            X_val_encoded = encoding_pipeline.transform(X_val)\n",
        "\n",
        "            # LGBM model - KHÔNG dùng early stopping trong CV\n",
        "            lgbm = LGBMRegressor(\n",
        "                boosting_type=boosting_type,\n",
        "                objective='regression',\n",
        "                learning_rate=learning_rate,\n",
        "                n_estimators=n_estimators,\n",
        "                max_depth=max_depth,\n",
        "                num_leaves=num_leaves,\n",
        "                min_child_samples=min_child_samples,\n",
        "                min_split_gain=min_split_gain,\n",
        "                colsample_bytree=colsample_bytree,\n",
        "                reg_alpha=reg_alpha,\n",
        "                reg_lambda=reg_lambda,\n",
        "                subsample=subsample,\n",
        "                subsample_freq=subsample_freq,\n",
        "                random_state=42,\n",
        "                n_jobs= 1,\n",
        "                verbosity=-1\n",
        "            )\n",
        "\n",
        "            # SỬA LẠI: Fit đơn giản không dùng early stopping trong CV\n",
        "            lgbm.fit(X_tr_encoded, y_tr_single)\n",
        "\n",
        "            y_pred_val = lgbm.predict(X_val_encoded)\n",
        "            result = evaluate(y_val_single, y_pred_val)\n",
        "            rmse = result['RMSE']\n",
        "            rmse_scores.append(rmse)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in trial: {e}\")\n",
        "            return float('inf')\n",
        "\n",
        "    mean_rmse = np.mean(rmse_scores)\n",
        "\n",
        "    # Log lên ClearML\n",
        "    logger_lgbm.report_scalar(\n",
        "        title=f'Optuna Tuning - LGBM [{target_name}]',\n",
        "        series='Trial RMSE',\n",
        "        value=mean_rmse,\n",
        "        iteration=trial.number\n",
        "    )\n",
        "\n",
        "    params_table = pd.DataFrame([{\n",
        "        \"target\": target_name,\n",
        "        \"boosting_type\": boosting_type,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"n_estimators\": n_estimators,\n",
        "        \"max_depth\": max_depth,\n",
        "        \"num_leaves\": num_leaves,\n",
        "        \"encoding_method\": encoding_method_condition,\n",
        "        \"n_seasons\": n_seasons,\n",
        "        \"n_quantiles\": n_quantiles\n",
        "    }])\n",
        "    logger_lgbm.report_table(\n",
        "        title=f\"Trial {trial.number} - {target_name}\",\n",
        "        series=\"params\",\n",
        "        iteration=trial.number,\n",
        "        table_plot=params_table\n",
        "    )\n",
        "\n",
        "    return mean_rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56069b0e",
      "metadata": {
        "id": "56069b0e"
      },
      "outputs": [],
      "source": [
        "# === 2. Hàm helper cho LGBM ===\n",
        "def extract_lgbm_params(params):\n",
        "    lgbm_params = {\n",
        "        'boosting_type': params['boosting_type'],\n",
        "        'objective': 'regression',\n",
        "        'learning_rate': params['learning_rate'],\n",
        "        'n_estimators': params['n_estimators'],\n",
        "        'max_depth': params['max_depth'],\n",
        "        'num_leaves': params['num_leaves'],\n",
        "        'min_child_samples': params['min_child_samples'],\n",
        "        'min_split_gain': params['min_split_gain'],\n",
        "        'colsample_bytree': params['colsample_bytree'],\n",
        "        'reg_alpha': params['reg_alpha'],\n",
        "        'reg_lambda': params['reg_lambda'],\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "        'verbosity': -1\n",
        "    }\n",
        "\n",
        "    # Conditional parameters\n",
        "    if params['boosting_type'] in ['goss', 'dart']:\n",
        "        lgbm_params['subsample'] = params['subsample']\n",
        "\n",
        "    if params['boosting_type'] == 'dart':\n",
        "        lgbm_params['subsample_freq'] = params['subsample_freq']\n",
        "\n",
        "    return lgbm_params\n",
        "\n",
        "def build_final_encoding_pipeline(params):\n",
        "    \"\"\"Build encoding pipeline với best parameters\"\"\"\n",
        "    return pl.build_encoding_pipeline(\n",
        "        is_category=params['conditions_is_category'],\n",
        "        encoding_method_condition=params['encoding_method_condition'],\n",
        "        n_seasons=params['n_seasons'],\n",
        "        n_quantiles=params['n_quantiles']\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c1e14d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "2c1e14d6",
        "outputId": "3bc933b8-6abe-49bd-9899-c6f1ae05b4ed"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'y_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m encoding_pipelines_per_target \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Tắt logging\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# optuna.logging.set_verbosity(optuna.logging.WARNING)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, target_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43my_train\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTuning LGBM for target: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(y_train\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Tạo study riêng cho mỗi target\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'y_train' is not defined"
          ]
        }
      ],
      "source": [
        "# === 3. Tune riêng cho từng target ===\n",
        "best_lgbm_models_per_target = {}\n",
        "best_lgbm_params_per_target = {}\n",
        "encoding_pipelines_per_target = {}\n",
        "\n",
        "# Tắt logging\n",
        "# optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "for idx, target_name in enumerate(y_train.columns):\n",
        "    print(f\"Tuning LGBM for target: {target_name} ({idx + 1}/{len(y_train.columns)})\")\n",
        "\n",
        "    # Tạo study riêng cho mỗi target\n",
        "    study = optuna.create_study(\n",
        "        direction='minimize',\n",
        "        sampler=optuna.samplers.TPESampler(seed=42)\n",
        "    )\n",
        "\n",
        "    study.optimize(\n",
        "        lambda trial: objective_lgbm_per_target(trial, target_name, idx),\n",
        "        n_trials= 50,\n",
        "        show_progress_bar=False,\n",
        "        callbacks=[lambda study, trial: None]\n",
        "    )\n",
        "\n",
        "    # Lưu best params\n",
        "    best_lgbm_params_per_target[target_name] = study.best_trial.params\n",
        "    best_lgbm_params_per_target[target_name]['target_idx'] = idx\n",
        "\n",
        "    # Build final encoding pipeline\n",
        "    encoding_pipeline = build_final_encoding_pipeline(study.best_trial.params)\n",
        "\n",
        "    # Override is_category cho từng component\n",
        "    encoding_pipeline.named_steps['season_encode'].is_category = study.best_trial.params['season_is_category']\n",
        "    encoding_pipeline.named_steps['wind_encode'].is_category = study.best_trial.params['wind_is_category']\n",
        "\n",
        "    # encoding trên toàn bộ training data\n",
        "    X_train_encoded = encoding_pipeline.fit_transform(\n",
        "        X_train,\n",
        "        y_train.iloc[:, idx] if len(y_train.shape) > 1 else y_train\n",
        "    )\n",
        "\n",
        "    #  train final LGBM model\n",
        "    best_lgbm = LGBMRegressor(**extract_lgbm_params(study.best_trial.params))\n",
        "\n",
        "    best_lgbm.fit(\n",
        "        X_train_encoded,\n",
        "        y_train.iloc[:, idx] if len(y_train.shape) > 1 else y_train\n",
        "    )\n",
        "\n",
        "    best_lgbm_models_per_target[target_name] = {\n",
        "        'model': best_lgbm,\n",
        "        'encoding_pipeline': encoding_pipeline\n",
        "    }\n",
        "\n",
        "    encoding_pipelines_per_target[target_name] = encoding_pipeline\n",
        "\n",
        "    # Log kết quả\n",
        "    logger_lgbm.report_scalar(\n",
        "        title='Best RMSE per Target - LGBM',\n",
        "        series=target_name,\n",
        "        value=study.best_value,\n",
        "        iteration=idx\n",
        "    )\n",
        "\n",
        "    print(f\" {target_name} - Best RMSE: {study.best_value:.4f}\")\n",
        "    print(f\" {target_name} - Best boosting: {study.best_trial.params['boosting_type']}\")\n",
        "    print(f\" {target_name} - Encoding: {study.best_trial.params['encoding_method_condition']}\")\n",
        "    print(f\" {target_name} - Seasons: {study.best_trial.params['n_seasons']}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# Khôi phục logging\n",
        "#optuna.logging.set_verbosity(optuna.logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d9a1786",
      "metadata": {
        "id": "3d9a1786"
      },
      "source": [
        "### Đoạn bên dưới có early stopping với prune cơ mà nó có lỗi error in trial gì đó, Hoàng check hoặc thay code mới của c cg dc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62727bc8",
      "metadata": {
        "id": "62727bc8"
      },
      "outputs": [],
      "source": [
        "from optuna.pruners import MedianPruner\n",
        "from lightgbm import early_stopping\n",
        "\n",
        "# === 1. Objective function cho LGBM với Early Stopping và Pruner ===\n",
        "def objective_lgbm_per_target(trial, target_name, target_idx):\n",
        "    # Hyperparameters cho encoding pipeline\n",
        "    encoding_method_condition = trial.suggest_categorical(\"encoding_method_condition\",\n",
        "                                                         [\"ordinal\", \"target\", \"quantile\"])\n",
        "    n_seasons = trial.suggest_int(\"n_seasons\", 3, 8)\n",
        "    n_quantiles = trial.suggest_int(\"n_quantiles\", 2, 6)\n",
        "\n",
        "    # Tune is_category cho từng encoder\n",
        "    conditions_is_category = trial.suggest_categorical(\"conditions_is_category\", [True, False])\n",
        "    season_is_category = trial.suggest_categorical(\"season_is_category\", [True, False])\n",
        "    wind_is_category = trial.suggest_categorical(\"wind_is_category\", [True, False])\n",
        "\n",
        "    # Hyperparameters cho LGBM\n",
        "    boosting_type = trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"dart\", \"goss\"])\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True)\n",
        "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000)\n",
        "    max_depth = trial.suggest_int(\"max_depth\", 3, 12)\n",
        "    num_leaves = trial.suggest_int(\"num_leaves\", 15, 255)\n",
        "    min_child_samples = trial.suggest_int(\"min_child_samples\", 5, 100)\n",
        "    min_split_gain = trial.suggest_float(\"min_split_gain\", 0.0, 1.0)\n",
        "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0)\n",
        "    reg_alpha = trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True)\n",
        "    reg_lambda = trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True)\n",
        "\n",
        "    # Conditional parameters\n",
        "    if boosting_type in ['goss', 'dart']:\n",
        "        subsample = trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
        "    else:\n",
        "        subsample = 1.0\n",
        "\n",
        "    if boosting_type == 'dart':\n",
        "        subsample_freq = trial.suggest_int(\"subsample_freq\", 1, 10)\n",
        "        drop_rate = trial.suggest_float(\"drop_rate\", 0.05, 0.5)\n",
        "        skip_drop = trial.suggest_float(\"skip_drop\", 0.3, 0.7)\n",
        "    else:\n",
        "        subsample_freq = 0\n",
        "        drop_rate = 0.1\n",
        "        skip_drop = 0.5\n",
        "\n",
        "    # TimeSeriesSplit\n",
        "    cv = TimeSeriesSplit(n_splits=5)\n",
        "    rmse_scores = []\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_train)):\n",
        "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        # Chỉ lấy target hiện tại\n",
        "        y_tr_single = y_tr.iloc[:, target_idx] if len(y_tr.shape) > 1 else y_tr\n",
        "        y_val_single = y_val.iloc[:, target_idx] if len(y_val.shape) > 1 else y_val\n",
        "\n",
        "        try:\n",
        "            # Build encoding pipeline với các parameters được tuning\n",
        "            encoding_pipeline = pl.build_encoding_pipeline(\n",
        "                is_category=conditions_is_category,\n",
        "                encoding_method_condition=encoding_method_condition,\n",
        "                n_seasons=n_seasons,\n",
        "                n_quantiles=n_quantiles\n",
        "            )\n",
        "\n",
        "            # Override is_category cho từng component\n",
        "            encoding_pipeline.named_steps['season_encode'].is_category = season_is_category\n",
        "            encoding_pipeline.named_steps['wind_encode'].is_category = wind_is_category\n",
        "\n",
        "            # Apply encoding\n",
        "            X_tr_encoded = encoding_pipeline.fit_transform(X_tr, y_tr_single)\n",
        "            X_val_encoded = encoding_pipeline.transform(X_val)\n",
        "\n",
        "            # LGBM model với early stopping\n",
        "            lgbm_params = {\n",
        "                'boosting_type': boosting_type,\n",
        "                'objective': 'regression',\n",
        "                'learning_rate': learning_rate,\n",
        "                'n_estimators': n_estimators,\n",
        "                'max_depth': max_depth,\n",
        "                'num_leaves': num_leaves,\n",
        "                'min_child_samples': min_child_samples,\n",
        "                'min_split_gain': min_split_gain,\n",
        "                'colsample_bytree': colsample_bytree,\n",
        "                'reg_alpha': reg_alpha,\n",
        "                'reg_lambda': reg_lambda,\n",
        "                'subsample': subsample,\n",
        "                'subsample_freq': subsample_freq,\n",
        "                'random_state': 42,\n",
        "                'n_jobs': 1,\n",
        "                'verbosity': -1\n",
        "            }\n",
        "\n",
        "            # Thêm params cho dart\n",
        "            if boosting_type == 'dart':\n",
        "                lgbm_params['drop_rate'] = drop_rate\n",
        "                lgbm_params['skip_drop'] = skip_drop\n",
        "\n",
        "            lgbm = LGBMRegressor(**lgbm_params)\n",
        "\n",
        "            # Fit với early stopping\n",
        "            lgbm.fit(\n",
        "                X_tr_encoded, y_tr_single,\n",
        "                eval_set=[(X_val_encoded, y_val_single)],\n",
        "                eval_metric='rmse',\n",
        "                callbacks=[early_stopping(stopping_rounds=50, verbose=False)]\n",
        "            )\n",
        "\n",
        "            y_pred_val = lgbm.predict(X_val_encoded)\n",
        "            result = evaluate(y_val_single, y_pred_val)\n",
        "            rmse = result['RMSE']\n",
        "            rmse_scores.append(rmse)\n",
        "\n",
        "            # Báo cho Optuna biết tiến độ trial để pruning\n",
        "            trial.report(rmse, step=fold_idx)\n",
        "            if trial.should_prune():\n",
        "                raise optuna.TrialPruned()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in trial: {e}\")\n",
        "            return float('inf')\n",
        "\n",
        "    mean_rmse = np.mean(rmse_scores)\n",
        "\n",
        "    # Log lên ClearML\n",
        "    logger_lgbm.report_scalar(\n",
        "        title=f'Optuna Tuning - LGBM [{target_name}]',\n",
        "        series='Trial RMSE',\n",
        "        value=mean_rmse,\n",
        "        iteration=trial.number\n",
        "    )\n",
        "\n",
        "    params_table = pd.DataFrame([{\n",
        "        \"target\": target_name,\n",
        "        \"boosting_type\": boosting_type,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"n_estimators\": n_estimators,\n",
        "        \"max_depth\": max_depth,\n",
        "        \"num_leaves\": num_leaves,\n",
        "        \"encoding_method\": encoding_method_condition,\n",
        "        \"n_seasons\": n_seasons,\n",
        "        \"n_quantiles\": n_quantiles\n",
        "    }])\n",
        "    logger_lgbm.report_table(\n",
        "        title=f\"Trial {trial.number} - {target_name}\",\n",
        "        series=\"params\",\n",
        "        iteration=trial.number,\n",
        "        table_plot=params_table\n",
        "    )\n",
        "\n",
        "    return mean_rmse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5e6d4e1",
      "metadata": {
        "id": "f5e6d4e1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === 2. Hàm helper cho LGBM (CẬP NHẬT) ===\n",
        "def extract_lgbm_params(params):\n",
        "    lgbm_params = {\n",
        "        'boosting_type': params['boosting_type'],\n",
        "        'objective': 'regression',\n",
        "        'learning_rate': params['learning_rate'],\n",
        "        'n_estimators': params['n_estimators'],\n",
        "        'max_depth': params['max_depth'],\n",
        "        'num_leaves': params['num_leaves'],\n",
        "        'min_child_samples': params['min_child_samples'],\n",
        "        'min_split_gain': params['min_split_gain'],\n",
        "        'colsample_bytree': params['colsample_bytree'],\n",
        "        'reg_alpha': params['reg_alpha'],\n",
        "        'reg_lambda': params['reg_lambda'],\n",
        "        'random_state': 42,\n",
        "        'n_jobs': 1,\n",
        "        'verbosity': -1\n",
        "    }\n",
        "\n",
        "    # Conditional parameters\n",
        "    if params['boosting_type'] in ['goss', 'dart']:\n",
        "        lgbm_params['subsample'] = params['subsample']\n",
        "\n",
        "    if params['boosting_type'] == 'dart':\n",
        "        lgbm_params['subsample_freq'] = params['subsample_freq']\n",
        "        lgbm_params['drop_rate'] = params.get('drop_rate', 0.1)\n",
        "        lgbm_params['skip_drop'] = params.get('skip_drop', 0.5)\n",
        "\n",
        "    return lgbm_params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c332cefa",
      "metadata": {
        "id": "c332cefa"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === 3. Tune riêng cho từng target với Pruner ===\n",
        "best_lgbm_models_per_target = {}\n",
        "best_lgbm_params_per_target = {}\n",
        "encoding_pipelines_per_target = {}\n",
        "\n",
        "# Cấu hình Pruner\n",
        "pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=2, interval_steps=1)\n",
        "\n",
        "# Tắt logging\n",
        "# optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "for idx, target_name in enumerate(y_train.columns):\n",
        "    print(f\"Tuning LGBM for target: {target_name} ({idx + 1}/{len(y_train.columns)})\")\n",
        "\n",
        "    # Tạo study riêng cho mỗi target với pruner\n",
        "    study = optuna.create_study(\n",
        "        direction='minimize',\n",
        "        sampler=optuna.samplers.TPESampler(seed=42),\n",
        "        pruner=pruner  # THÊM PRUNER\n",
        "    )\n",
        "\n",
        "    study.optimize(\n",
        "        lambda trial: objective_lgbm_per_target(trial, target_name, idx),\n",
        "        n_trials= 50,\n",
        "        show_progress_bar=False,\n",
        "        callbacks=[lambda study, trial: None]\n",
        "    )\n",
        "\n",
        "    # Lưu best params\n",
        "    best_lgbm_params_per_target[target_name] = study.best_trial.params\n",
        "    best_lgbm_params_per_target[target_name]['target_idx'] = idx\n",
        "\n",
        "    # Build final encoding pipeline\n",
        "    encoding_pipeline = build_final_encoding_pipeline(study.best_trial.params)\n",
        "\n",
        "    # Override is_category cho từng component\n",
        "    encoding_pipeline.named_steps['season_encode'].is_category = study.best_trial.params['season_is_category']\n",
        "    encoding_pipeline.named_steps['wind_encode'].is_category = study.best_trial.params['wind_is_category']\n",
        "\n",
        "    # Apply encoding trên toàn bộ training data\n",
        "    X_train_encoded = encoding_pipeline.fit_transform(\n",
        "        X_train,\n",
        "        y_train.iloc[:, idx] if len(y_train.shape) > 1 else y_train\n",
        "    )\n",
        "\n",
        "    # Build và train final LGBM model với early stopping\n",
        "    best_lgbm = LGBMRegressor(**extract_lgbm_params(study.best_trial.params))\n",
        "\n",
        "    # Chia train/val cho early stopping\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_tr_final, X_val_final, y_tr_final, y_val_final = train_test_split(\n",
        "        X_train_encoded,\n",
        "        y_train.iloc[:, idx] if len(y_train.shape) > 1 else y_train,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        shuffle=False  # Giữ thứ tự time series\n",
        "    )\n",
        "\n",
        "    best_lgbm.fit(\n",
        "        X_tr_final, y_tr_final,\n",
        "        eval_set=[(X_val_final, y_val_final)],\n",
        "        eval_metric='rmse',\n",
        "        callbacks=[early_stopping(stopping_rounds=50, verbose=False)]\n",
        "    )\n",
        "\n",
        "    best_lgbm_models_per_target[target_name] = {\n",
        "        'model': best_lgbm,\n",
        "        'encoding_pipeline': encoding_pipeline\n",
        "    }\n",
        "\n",
        "    encoding_pipelines_per_target[target_name] = encoding_pipeline\n",
        "\n",
        "    # Log kết quả\n",
        "    logger_lgbm.report_scalar(\n",
        "        title='Best RMSE per Target - LGBM',\n",
        "        series=target_name,\n",
        "        value=study.best_value,\n",
        "        iteration=idx\n",
        "    )\n",
        "\n",
        "\n",
        "    # SỬA PHẦN NÀY: Thêm thông tin is_category vào print\n",
        "    print(f\" {target_name} - Best RMSE: {study.best_value:.4f}\")\n",
        "    print(f\" {target_name} - Best boosting: {study.best_trial.params['boosting_type']}\")\n",
        "    print(f\" {target_name} - Encoding: {study.best_trial.params['encoding_method_condition']}\")\n",
        "    print(f\" {target_name} - Seasons: {study.best_trial.params['n_seasons']}\")\n",
        "    print(f\" {target_name} - Conditions is_category: {study.best_trial.params['conditions_is_category']}\")\n",
        "    print(f\" {target_name} - Season is_category: {study.best_trial.params['season_is_category']}\")\n",
        "    print(f\" {target_name} - Wind is_category: {study.best_trial.params['wind_is_category']}\")\n",
        "    print(f\" {target_name} - Quantiles: {study.best_trial.params['n_quantiles']}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Khôi phục logging\n",
        "#optuna.logging.set_verbosity(optuna.logging.INFO)\n",
        "\n",
        "# In summary tất cả parameters\n",
        "print(\"\\n=== BEST PARAMETERS SUMMARY ===\")\n",
        "for target_name in y_train.columns:\n",
        "    params = best_lgbm_params_per_target[target_name]\n",
        "    print(f\"\\n{target_name}:\")\n",
        "    print(f\"  RMSE: {study.best_value:.4f}\")\n",
        "    print(f\"  Boosting: {params['boosting_type']}\")\n",
        "    print(f\"  Encoding: {params['encoding_method_condition']}\")\n",
        "    print(f\"  Seasons: {params['n_seasons']}\")\n",
        "    print(f\"  Quantiles: {params['n_quantiles']}\")\n",
        "    print(f\"  Conditions categorical: {params['conditions_is_category']}\")\n",
        "    print(f\"  Season categorical: {params['season_is_category']}\")\n",
        "    print(f\"  Wind categorical: {params['wind_is_category']}\")\n",
        "    print(f\"  Learning rate: {params['learning_rate']:.4f}\")\n",
        "    print(f\"  Max depth: {params['max_depth']}\")\n",
        "    print(f\"  Num leaves: {params['num_leaves']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91efdecb",
      "metadata": {
        "id": "91efdecb"
      },
      "source": [
        "#### C. Training Final Model với hyper parameter và đánh giá cuối cùng trên Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6636f14c",
      "metadata": {
        "id": "6636f14c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === 4. Tạo MultiOutput wrapper cho LGBM ===\n",
        "class CustomLGBMMultiOutputRegressor:\n",
        "    def __init__(self, models_per_target):\n",
        "        self.models_per_target = models_per_target\n",
        "        self.target_names = list(models_per_target.keys())\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for target_name, model_info in self.models_per_target.items():\n",
        "            # Apply encoding\n",
        "            X_encoded = model_info['encoding_pipeline'].transform(X)\n",
        "            # Predict\n",
        "            pred = model_info['model'].predict(X_encoded).reshape(-1, 1)\n",
        "            predictions.append(pred)\n",
        "        return np.hstack(predictions)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {\"models_per_target\": self.models_per_target}\n",
        "\n",
        "# Tạo final model\n",
        "final_lgbm_multi_model = CustomLGBMMultiOutputRegressor(best_lgbm_models_per_target)\n",
        "\n",
        "# === 5. Đánh giá final model ===\n",
        "y_pred_test_lgbm = final_lgbm_multi_model.predict(X_test)\n",
        "y_pred_train_lgbm = final_lgbm_multi_model.predict(X_train)\n",
        "\n",
        "final_test_metrics_lgbm = evaluate_multi_output(y_test, y_pred_test_lgbm)\n",
        "final_train_metrics_lgbm = evaluate_multi_output(y_train, y_pred_train_lgbm)\n",
        "\n",
        "print(\"LGBM Final Model - Test metrics (average):\", final_test_metrics_lgbm[\"average\"])\n",
        "print(\"LGBM Final Model - Test metrics (perday):\", final_test_metrics_lgbm[\"per_day\"])\n",
        "print()\n",
        "print(\"LGBM Final Model - Train metrics (average):\", final_train_metrics_lgbm[\"average\"])\n",
        "print(\"LGBM Final Model - Train metrics (perday):\", final_train_metrics_lgbm[\"per_day\"])\n",
        "\n",
        "# === 6. Log final results ===\n",
        "task_lgbm.upload_artifact(\"Best LGBM Parameters Per Target\", best_lgbm_params_per_target)\n",
        "task_lgbm.upload_artifact(\"LGBM Final Model - Test Metrics\", final_test_metrics_lgbm)\n",
        "task_lgbm.upload_artifact(\"LGBM Final Model - Train Metrics\", final_train_metrics_lgbm)\n",
        "\n",
        "# === 7. Lưu models ===\n",
        "lgbm_models_path = r\"models/LGBM_models_per_target.pkl\"\n",
        "joblib.dump({\n",
        "    'models': best_lgbm_models_per_target,\n",
        "    'params': best_lgbm_params_per_target,\n",
        "    'final_model': final_lgbm_multi_model\n",
        "}, lgbm_models_path)\n",
        "\n",
        "print(\"=== LGBM TUNING COMPLETED ===\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46809134",
      "metadata": {
        "id": "46809134"
      },
      "source": [
        "#### T bảo, nếu két quả tuning tệ hơn tune_lgb thì lấy cái tune_lgb cũ nhé (kết quả tệ cũng có thể do mình drop hết base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31a2a827",
      "metadata": {
        "id": "31a2a827"
      },
      "outputs": [],
      "source": [
        "# Defaut Goss\n",
        "tune_lgb = LGBMRegressor(\n",
        "    boosting_type='goss',\n",
        "    objective= 'regression', # loss function\n",
        "    colsample_bytree=0.7688738230630878,\n",
        "    learning_rate=0.015433744772417535,\n",
        "    max_depth= 4,\n",
        "    min_child_samples=50,\n",
        "    min_split_gain=0.4084759794499262,\n",
        "    n_estimators=417,\n",
        "    num_leaves=128,\n",
        "    reg_alpha=0.008,\n",
        "    reg_lambda=0.008,\n",
        "    # subsample=0.9098920662929666,\n",
        "    # subsample_freq= 10,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "model_grad = MultiOutputRegressor(estimator=tune_lgb, n_jobs=-1)\n",
        "\n",
        "# 2️⃣ Time series cross-validation (chỉ trên tập train)\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "cv_scores = []  # lưu kết quả trung bình của mỗi fold\n",
        "\n",
        "print(\"=== Time Series Cross-Validation ===\")\n",
        "\n",
        "fold_idx = 1\n",
        "for train_idx, val_idx in tscv.split(X_train):\n",
        "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "    # Huấn luyện mô hình\n",
        "    model_grad.fit(X_tr, y_tr)\n",
        "    y_pred_val = model_grad.predict(X_val)\n",
        "\n",
        "    # Đánh giá\n",
        "    metrics_val = evaluate_multi_output(y_val, y_pred_val)\n",
        "    avg_val = metrics_val[\"average\"]\n",
        "    cv_scores.append(avg_val)\n",
        "\n",
        "    print(f\"\\nFold {fold_idx} validation metrics:\")\n",
        "    print(avg_val)\n",
        "    fold_idx += 1\n",
        "\n",
        "# 3️⃣ Tổng hợp kết quả cross-validation\n",
        "cv_df = pd.DataFrame(cv_scores)\n",
        "print(\"\\n=====> Cross-validation (Time Series) trung bình:\")\n",
        "print(cv_df.mean())\n",
        "\n",
        "print(\"\\n=====> Cross-validation (Time Series) độ lệch chuẩn (variance):\")\n",
        "print(cv_df.std())\n",
        "\n",
        "# Sau khi bạn thấy mô hình ổn định qua các fold,\n",
        "# bạn huấn luyện lại trên toàn bộ tập train rồi đánh giá 1 lần trên test (dữ liệu tương lai thật).\n",
        "\n",
        "# 4️⃣ Huấn luyện lại toàn bộ tập train sau khi CV\n",
        "model_grad.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# 5️⃣ Đánh giá cuối cùng trên tập train\n",
        "y_pred_train = model_grad.predict(X_train)\n",
        "defGrad_train_metrics = evaluate_multi_output(y_train, y_pred_train)\n",
        "\n",
        "print(\"\\n=====> Train metrics (trung bình):\", defGrad_train_metrics[\"average\"])\n",
        "print(\"Train metrics (chi tiết từng ngày):\", defGrad_train_metrics[\"per_day\"])\n",
        "\n",
        "\n",
        "# 5️⃣ Đánh giá cuối cùng trên tập test\n",
        "y_pred_test = model_grad.predict(X_test)\n",
        "defGrad_test_metrics = evaluate_multi_output(y_test, y_pred_test)\n",
        "\n",
        "print(\"\\n=====> Test metrics (trung bình):\", defGrad_test_metrics[\"average\"])\n",
        "print(\"Test metrics (chi tiết từng ngày):\", defGrad_test_metrics[\"per_day\"])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54289a69",
      "metadata": {
        "id": "54289a69"
      },
      "outputs": [],
      "source": [
        "# Sau khi bạn thấy mô hình ổn định qua các fold,\n",
        "# bạn huấn luyện lại trên toàn bộ tập train rồi đánh giá 1 lần trên test (dữ liệu tương lai thật).\n",
        "\n",
        "# 4️⃣ Huấn luyện lại toàn bộ tập train sau khi CV\n",
        "model_grad.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# 5️⃣ Đánh giá cuối cùng trên tập train\n",
        "y_pred_train = model_grad.predict(X_train)\n",
        "defGrad_train_metrics = evaluate_multi_output(y_train, y_pred_train)\n",
        "\n",
        "print(\"\\n=====> Train metrics (trung bình):\", defGrad_train_metrics[\"average\"])\n",
        "print(\"Train metrics (chi tiết từng ngày):\", defGrad_train_metrics[\"per_day\"])\n",
        "\n",
        "\n",
        "# 5️⃣ Đánh giá cuối cùng trên tập test\n",
        "y_pred_test = model_grad.predict(X_test)\n",
        "defGrad_test_metrics = evaluate_multi_output(y_test, y_pred_test)\n",
        "\n",
        "print(\"\\n=====> Test metrics (trung bình):\", defGrad_test_metrics[\"average\"])\n",
        "print(\"Test metrics (chi tiết từng ngày):\", defGrad_test_metrics[\"per_day\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c94032e",
      "metadata": {
        "id": "0c94032e"
      },
      "source": [
        "### 3. Bảng so sánh kết quả 2 model (so sánh chung chung để chốt chọn model nào - lgb - sau đó mình mới phân tích cụ thể kết quả ở mục sau)\n",
        "- Nhàn lên cho t cái form so sánh những gì (các chỉ số average, perday, time) - Hoặc chụp ảnh dùng cái chức năng so sánh của ClearML - Hoặc cả 2\n",
        "- Xong Nhàn với Hoàng train ra được model tốt nhất của linear với LGB rồi thì điền vào đây nhé (mấy cái metric căn bản có lưu trên clearml)\n",
        "- Kết luận: Chọn model LGB"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5714134",
      "metadata": {
        "id": "c5714134"
      },
      "source": [
        "### 4. Đánh giá và Giải thích cụ thể kết quả của model (LGB) - Hoàng\n",
        "bn chờ t cập nhật format biểu đồ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f17955db",
      "metadata": {
        "id": "f17955db"
      },
      "outputs": [],
      "source": [
        "# Giả sử y_test có index datetime\n",
        "y_test.index = pd.to_datetime(y_test.index)\n",
        "\n",
        "# Tạo DataFrame/Series cho y_pred với index từ y_test\n",
        "y_pred_with_index = pd.DataFrame(y_pred_test, index=y_test.index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a98206f",
      "metadata": {
        "id": "4a98206f"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_separate_horizons(y_test, y_pred_test, figsize=(16, 6)):\n",
        "    \"\"\"\n",
        "    Vẽ 5 biểu đồ riêng biệt cho 5 horizons.\n",
        "    Mỗi biểu đồ hiển thị dữ liệu thực (y_true) và dự đoán (y_pred)\n",
        "    với màu và kiểu đường dễ phân biệt.\n",
        "    \"\"\"\n",
        "\n",
        "    # Kiểm tra input\n",
        "    assert y_test.shape[1] == 5, \"y_test phải có 5 columns\"\n",
        "    assert y_pred_test.shape[1] == 5, \"y_pred_test phải có 5 columns\"\n",
        "    assert len(y_test) == len(y_pred_test), \"Số samples phải khớp\"\n",
        "\n",
        "    horizon_names = [\n",
        "        'Horizon 1 (Next Day)',\n",
        "        'Horizon 2 (2 Days Ahead)',\n",
        "        'Horizon 3 (3 Days Ahead)',\n",
        "        'Horizon 4 (4 Days Ahead)',\n",
        "        'Horizon 5 (5 Days Ahead)'\n",
        "    ]\n",
        "\n",
        "    # Màu nhẹ cho Actual, màu đậm cho Predict\n",
        "    true_color = \"#1f77b4\"   # Xanh dương dịu\n",
        "    pred_color = \"#ff7f0e\"   # Cam dịu\n",
        "\n",
        "    for i in range(5):\n",
        "        plt.figure(figsize=figsize)\n",
        "\n",
        "        y_true_horizon = y_test.iloc[:, i]\n",
        "        y_pred_horizon = y_pred_test[:, i]\n",
        "        dates = y_test.index\n",
        "\n",
        "        # Metrics\n",
        "        rmse = np.sqrt(np.mean((y_true_horizon - y_pred_horizon) ** 2))\n",
        "        mae = np.mean(np.abs(y_true_horizon - y_pred_horizon))\n",
        "        r2 = 1 - np.sum((y_true_horizon - y_pred_horizon) ** 2) / np.sum((y_true_horizon - np.mean(y_true_horizon)) ** 2)\n",
        "\n",
        "        # --- Vẽ biểu đồ ---\n",
        "        plt.plot(\n",
        "            dates, y_true_horizon,\n",
        "            label='Actual Temperature',\n",
        "            color=true_color,\n",
        "            linewidth=1\n",
        "        )\n",
        "\n",
        "        plt.plot(\n",
        "            dates, y_pred_horizon,\n",
        "            label='Predicted Temperature',\n",
        "            color=pred_color,\n",
        "            linewidth=1\n",
        "        )\n",
        "\n",
        "        # --- Tuỳ chỉnh ---\n",
        "        plt.title(\n",
        "            f'{horizon_names[i]}\\nRMSE: {rmse:.2f}°C | MAE: {mae:.2f}°C | R²: {r2:.3f}',\n",
        "            fontsize=13, fontweight='bold', pad=20\n",
        "        )\n",
        "        plt.ylabel('Temperature (°C)', fontsize=11)\n",
        "        plt.xlabel('Date', fontsize=11)\n",
        "        plt.legend(fontsize=11, frameon=False)\n",
        "        plt.grid(True, linestyle=':', alpha=0.5)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "plot_separate_horizons(y_test, y_pred_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32f45c9a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "32f45c9a",
        "outputId": "1fba150c-100c-4776-d116-e08a26226a20"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'y_test' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4030441546.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mplot_rmse_over_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
          ]
        }
      ],
      "source": [
        "def plot_rmse_over_time(y_test, y_pred_test, window=7, figsize=(14, 5)):\n",
        "    \"\"\"\n",
        "    Vẽ 5 biểu đồ riêng biệt cho RMSE theo thời gian (rolling window),\n",
        "    với cùng đơn vị (cố định trục Y).\n",
        "    \"\"\"\n",
        "\n",
        "    # Kiểm tra input\n",
        "    assert y_test.shape[1] == 5, \"y_test phải có 5 columns\"\n",
        "    assert y_pred_test.shape[1] == 5, \"y_pred_test phải có 5 columns\"\n",
        "    assert len(y_test) == len(y_pred_test), \"Số samples phải khớp\"\n",
        "\n",
        "    horizon_names = [\n",
        "        'Horizon 1 (Next Day)',\n",
        "        'Horizon 2 (2 Days Ahead)',\n",
        "        'Horizon 3 (3 Days Ahead)',\n",
        "        'Horizon 4 (4 Days Ahead)',\n",
        "        'Horizon 5 (5 Days Ahead)'\n",
        "    ]\n",
        "\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
        "    dates = y_test.index\n",
        "\n",
        "    # Tính rolling RMSE cho tất cả horizons để tìm giới hạn trục Y chung\n",
        "    all_rmse = []\n",
        "    for i in range(5):\n",
        "        y_true = y_test.iloc[:, i]\n",
        "        y_pred = y_pred_test[:, i]\n",
        "        rolling_rmse = np.sqrt(((y_true - y_pred)**2).rolling(window).mean())\n",
        "        all_rmse.append(rolling_rmse)\n",
        "\n",
        "    # Tính min/max chung cho toàn bộ 5 biểu đồ\n",
        "    all_values = pd.concat(all_rmse)\n",
        "    ymin, ymax = all_values.min(), all_values.max()\n",
        "    margin = (ymax - ymin) * 0.1  # thêm khoảng trống 10%\n",
        "    ymin -= margin\n",
        "    ymax += margin\n",
        "\n",
        "    # Vẽ từng biểu đồ\n",
        "    for i in range(5):\n",
        "        plt.figure(figsize=figsize)\n",
        "        plt.plot(dates, all_rmse[i], color=colors[i], linewidth=1.8)\n",
        "\n",
        "        mean_rmse = all_rmse[i].mean()\n",
        "        plt.axhline(mean_rmse, color='gray', linestyle='--', linewidth=1,\n",
        "                    label=f'Mean RMSE = {mean_rmse:.2f}')\n",
        "\n",
        "        plt.title(f'{horizon_names[i]} - Rolling RMSE (window={window} days)',\n",
        "                  fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Date', fontsize=12)\n",
        "        plt.ylabel('RMSE (°C)', fontsize=12)\n",
        "        plt.ylim(ymin, ymax)  # <-- cố định trục Y\n",
        "        plt.legend(fontsize=10, frameon=False)\n",
        "        plt.grid(True, linestyle=':', alpha=0.5)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "plot_rmse_over_time(y_test, y_pred_test, window=7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed07c359",
      "metadata": {
        "id": "ed07c359"
      },
      "source": [
        "### 5. UI + Model Maintainence + Realtime predict - Sương Mai\n",
        "- Sử dụng Gradio\n",
        "- Trả lời ngắn gọn câu hỏi của thầy \"\"The common sense is, if you train model and use it to predict day by day, at some point, the performance will downgrade. When you should retrain your model?\"\"\n",
        "<br> Lí do phải retrain là gì: ngắn gọn\n",
        "<br> Ngắn gọn cơ chế maintainnence: có 2 điều kiện khi RMSE vướt ngưỡng + 90 ngày maintain một lần\n",
        "\n",
        "- Sửa lại READ.ME cái mục cấu trúc project + Vẽ Biểu đồ (trên Canva,... tuỳ) về cấu trúc tổng thể, cách hoạt động của các file (VD như File Monitoring vẽ if/ else block Yes điều kiện retrain => mũi tên file Model_training, vv... )\n",
        "\n",
        "- Link Video sử dụng UI để dự đoán realtime + show Cloud trên ClearML của mình + kèm link"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df7ce3f2",
      "metadata": {
        "id": "df7ce3f2"
      },
      "source": [
        "### 6. Preprocessing/ Feature Enginering/ Tuning/ Training / Đánh giá kết quả Hourly (dùng luôn lgb)\n",
        "- Vẽ tổng quát Biểu đồ các step: input, output đầu ra mỗi step, thứ từ các step mũi tên như nào (Vẽ trên canva hay ppt r cop lại đây)\n",
        "- Preprocessing:\n",
        "   + Input: dữ liệu hourly bao nhiêu row, bao cột\n",
        "   + Drop columns nào, làm gì, ... (Nói là LGB nên cũng không cần xử lí nhiều, chủ yếu bỏ mấy cột thừa)\n",
        "   + Output: ...\n",
        "- Feature Engineering:\n",
        "   + Feature Engineering Hourly:\n",
        "      - Input: từ Preprocessing (đang bao nhiêu row hourly, bao nhiêu column)\n",
        "      - Giải thích qua làm gì:\n",
        "      - Output: (Còn bao nhiêu cột Daily, Tạo bao nhiêu columns: ngắn gọn cho t với nhóm nhiệt độ thì lấy aggregate mean, max, min, với theo chu kì trong ngày, Nhóm mặt trời thì chỉ lấy mean, với cao nhất trong ngày, ... vv)\n",
        "   + Feature Engineering Daily Adjusted to new feature in Hourly:\n",
        "      - Input:\n",
        "      - Giải thích qua làm gì, thêm cái gì đối với feature mới từ hourly\n",
        "      - Output:\n",
        "\n",
        "- Tuning/ Training:\n",
        "   + Cho t cái bản tóm tắt metric so sánh (Before training chỉ số các fold, Sau training kết quả train/ test average, perday so sánh với Daily LGB luôn)\n",
        "   + Trực quan hoá bằng biểu đồ đường, biểu đồ cột so sánh các metric RMSE, MAE, R2 nhìn cho dễ - đừng vẽ mỗi cái một dòng khó nhìn lắm, để hết vào 1 cái khung figure, rồi chia row, column trong đấy để biểu đồ vào\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbcf6357",
      "metadata": {
        "id": "cbcf6357"
      },
      "source": [
        "### 7. ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae3682e0",
      "metadata": {
        "id": "ae3682e0"
      },
      "outputs": [],
      "source": [
        "def lưu_nháp_kệ_đi():\n",
        "   pass\n",
        "   # hàm encoding conditions\n",
        "   # condition = dp.ConditionsEncoder(is_category = False, encoding_method='target') # False tốt hơn\n",
        "   # X_train = condition.fit_transform(X_train, X_train['temp'])\n",
        "   # X_test = condition.transform(X_test)\n",
        "\n",
        "   # # hàm encoding season\n",
        "   # season = dp.SeasonClassifier(n_seasons=5, is_category = False) # False tốt hơn\n",
        "   # X_train = season.fit_transform(X_train, X_train['temp'])\n",
        "   # X_test = season.transform(X_test)\n",
        "\n",
        "   # # hàm encoding wind\n",
        "   # wind = dp.WindCategoryEncoder(is_category= False, n_quantiles= 4) # nếu không dùng kết hợp với condtions thì False tốt hơn\n",
        "   # X_train = wind.fit_transform(X_train, X_train['temp'])\n",
        "   # X_test = wind.transform(X_test)\n",
        "\n",
        "   # # Nháp của t kệ đi\n",
        "   # # Chạy code so sánh Numeric season vs Category season\n",
        "   # # hàm encoding conditions\n",
        "   # condition = dp.ConditionsEncoder(is_category = True, encoding_method='target') # False tốt hơn\n",
        "   # train_feat = condition.fit_transform(train_feat, train_feat['temp'])\n",
        "   # test_feat = condition.transform(test_feat)\n",
        "\n",
        "   # # hàm encoding season\n",
        "   # season = dp.SeasonClassifier(n_seasons=5, is_category = True) # False tốt hơn\n",
        "   # train_feat = season.fit_transform(train_feat, train_feat['temp'])\n",
        "   # test_feat = season.transform(test_feat)\n",
        "\n",
        "   # # hàm encoding wind\n",
        "   # wind = dp.WindCategoryEncoder(is_category= True) # nếu không dùng kết hợp với condtions thì False tốt hơn\n",
        "   # train_feat = wind.fit_transform(train_feat, train_feat['temp'])\n",
        "   # test_feat = wind.transform(test_feat)\n",
        "\n",
        "   # # Chia X, y riêng biệt\n",
        "   # X_train = train_feat.drop(columns= target_col)\n",
        "   # y_train = train_feat[target_col]\n",
        "\n",
        "   # X_test = test_feat.drop(columns= target_col)\n",
        "   # y_test = test_feat[target_col]\n",
        "\n",
        "   # print(f\"Train: {X_train.shape, y_train.shape}, Test: {X_test.shape, y_test.shape}\")\n",
        "\n",
        "   # # Chạy code so sánh Numeric wind_cateogry vs Category wind_category\n",
        "\n",
        "\n",
        "   # # Chạy code so sánh Numeric conditions vs Category conditions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   # from optuna.pruners import MedianPruner\n",
        "   # from lightgbm import early_stopping\n",
        "\n",
        "   # # ===  Tạo hàm objective ===\n",
        "   # def objective(trial):\n",
        "   #     boosting_type = trial.suggest_categorical('boosting_type', ['gbdt', 'dart'])\n",
        "\n",
        "   #     params = {\n",
        "   #         'boosting_type': boosting_type,\n",
        "   #         'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
        "   #         'min_child_weight': trial.suggest_float('min_child_weight', 1e-3, 10.0, log=True),\n",
        "   #         'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 0.3),\n",
        "   #         'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "   #         'reg_alpha': trial.suggest_float('reg_alpha', 1e-5, 1.0, log=True),\n",
        "   #         'reg_lambda': trial.suggest_float('reg_lambda', 1e-5, 1.0, log=True),\n",
        "   #         'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "   #         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "   #         'subsample_freq': trial.suggest_int('subsample_freq', 1, 7),\n",
        "   #         'n_estimators': trial.suggest_int('n_estimators', 100, 1500),\n",
        "   #         'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
        "   #         'max_bin': trial.suggest_int('max_bin', 64, 512),\n",
        "   #         'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
        "\n",
        "   #         'objective': 'regression',\n",
        "   #         'metric': 'rmse',\n",
        "   #         'random_state': 42,\n",
        "   #         'n_jobs': -1,\n",
        "   #         'verbosity': -1  #\n",
        "   #     }\n",
        "\n",
        "   #     if boosting_type == 'dart':\n",
        "   #         params['drop_rate'] = trial.suggest_float('drop_rate', 0.05, 0.5)\n",
        "   #         params['skip_drop'] = trial.suggest_float('skip_drop', 0.3, 0.7)\n",
        "\n",
        "   #     # === TimeSeriesSplit CV ===\n",
        "   #     cv = TimeSeriesSplit(n_splits=5)\n",
        "   #     rmse_scores = []\n",
        "\n",
        "   #     for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_train_sel)):\n",
        "   #         X_tr, X_val = X_train_sel.iloc[train_idx], X_train_sel.iloc[val_idx]\n",
        "   #         y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "   #         # Fit riêng cho từng cột output\n",
        "   #         models_fold = {}\n",
        "   #         y_pred_val = pd.DataFrame(index=y_val.index, columns=y_val.columns)\n",
        "\n",
        "   #         for col in y_train.columns:\n",
        "   #             model = LGBMRegressor(**params)\n",
        "   #             model.fit(\n",
        "   #                 X_tr, y_tr[col],\n",
        "   #                 eval_set=[(X_val, y_val[col])],\n",
        "   #                 eval_metric='rmse',\n",
        "   #                 callbacks=[early_stopping(stopping_rounds=50, verbose=False)]\n",
        "\n",
        "   #             )\n",
        "   #             models_fold[col] = model\n",
        "   #             y_pred_val[col] = model.predict(X_val)\n",
        "\n",
        "   #         metrics = evaluate_multi_output(y_val, y_pred_val)\n",
        "   #         rmse_fold = metrics[\"average\"][\"RMSE\"]\n",
        "   #         rmse_scores.append(rmse_fold)\n",
        "\n",
        "   #         # Báo cho Optuna biết tiến độ trial\n",
        "   #         trial.report(rmse_fold, step=fold_idx)\n",
        "   #         if trial.should_prune():\n",
        "   #             raise optuna.TrialPruned()\n",
        "\n",
        "   #     mean_rmse = np.mean(rmse_scores)\n",
        "   #     return mean_rmse\n",
        "\n",
        "   # # === 3️ Cấu hình Pruner ===\n",
        "   # pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=2, interval_steps=1)\n",
        "\n",
        "   # # === 4 Chạy Optuna Study ===\n",
        "   # sampler = optuna.samplers.TPESampler(seed=42)\n",
        "   # study_grad = optuna.create_study(direction='minimize', sampler=sampler, pruner=pruner)\n",
        "   # study_grad.optimize(objective, n_trials=500, show_progress_bar=True)\n",
        "\n",
        "   # # === 5️ In kết quả ===\n",
        "   # print(\" Best parameters:\", study_grad.best_trial.params)\n",
        "   # print(\" Best RMSE (Optuna CV):\", study_grad.best_value)\n",
        "\n",
        "   # # === 6️ Fit lại với best params cho từng output ===\n",
        "   # best_params = study_grad.best_trial.params\n",
        "   # final_models = {}\n",
        "   # y_pred_train = pd.DataFrame(index=y_train.index, columns=y_train.columns)\n",
        "   # y_pred_test = pd.DataFrame(index=y_test.index, columns=y_test.columns)\n",
        "\n",
        "   # for col in y_train.columns:\n",
        "   #     model = LGBMRegressor(**best_params)\n",
        "   #     model.fit(\n",
        "   #         X_train_sel, y_train[col],\n",
        "   #         eval_set=[(X_test_sel, y_test[col])],\n",
        "   #         eval_metric='rmse',\n",
        "   #         callbacks=[early_stopping(stopping_rounds=50, verbose=False)]\n",
        "   #     )\n",
        "\n",
        "   #     final_models[col] = model\n",
        "   #     y_pred_train[col] = model.predict(X_train_sel)\n",
        "   #     y_pred_test[col] = model.predict(X_test_sel)\n",
        "\n",
        "   # # Evaluate\n",
        "   # bestGrad_train_metrics = evaluate_multi_output(y_train, y_pred_train)\n",
        "   # bestGrad_test_metrics = evaluate_multi_output(y_test, y_pred_test)\n",
        "\n",
        "   # print(\"Best Model - Train metrics (average):\", bestGrad_train_metrics[\"average\"])\n",
        "   # print(\"Best Model - Test metrics (average):\", bestGrad_test_metrics[\"average\"])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
